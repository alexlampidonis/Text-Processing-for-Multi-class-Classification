{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QPeANwYCkRMC"
      },
      "source": [
        "## Training Data Cleaning\n",
        "Download and load the dataset. There are some issues with the training split of the data that would stop it being\n",
        "used to train a classifier. Report all issues and how you fixed them."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LKyk3rOIYEJ9"
      },
      "source": [
        "Let's import the dataset and print the training split."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NPWnR9k8mS1-",
        "outputId": "20104c9f-a993-4b3b-f3c7-f2d0caf0fe2e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-06-06 17:02:05--  https://tinyurl.com/tadarchives\n",
            "Resolving tinyurl.com (tinyurl.com)... 104.18.111.161, 104.17.112.233, 2606:4700::6812:6fa1, ...\n",
            "Connecting to tinyurl.com (tinyurl.com)|104.18.111.161|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: https://gla-my.sharepoint.com/:u:/g/personal/jake_lever_glasgow_ac_uk/EcsUMcfLirhOkodUHhTLfQEBlLKRv2nI0skIGhZqa5KokQ?download=1 [following]\n",
            "--2025-06-06 17:02:05--  https://gla-my.sharepoint.com/:u:/g/personal/jake_lever_glasgow_ac_uk/EcsUMcfLirhOkodUHhTLfQEBlLKRv2nI0skIGhZqa5KokQ?download=1\n",
            "Resolving gla-my.sharepoint.com (gla-my.sharepoint.com)... 13.107.136.10, 13.107.138.10, 2620:1ec:8f8::10, ...\n",
            "Connecting to gla-my.sharepoint.com (gla-my.sharepoint.com)|13.107.136.10|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: /personal/jake_lever_glasgow_ac_uk/Documents/Teaching/tad_coursework_archives.zip?ga=1 [following]\n",
            "--2025-06-06 17:02:06--  https://gla-my.sharepoint.com/personal/jake_lever_glasgow_ac_uk/Documents/Teaching/tad_coursework_archives.zip?ga=1\n",
            "Reusing existing connection to gla-my.sharepoint.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 179728 (176K) [application/x-zip-compressed]\n",
            "Saving to: \u2018dataset.zip\u2019\n",
            "\n",
            "dataset.zip         100%[===================>] 175.52K   412KB/s    in 0.4s    \n",
            "\n",
            "2025-06-06 17:02:06 (412 KB/s) - \u2018dataset.zip\u2019 saved [179728/179728]\n",
            "\n",
            "Archive:  dataset.zip\n",
            "  inflating: dataset/dataset.json    \n",
            "  inflating: dataset/llm_prompt_template_1.json  \n",
            "  inflating: dataset/llm_prompt_template_2.json  \n",
            "  inflating: dataset/llm_prompt_template_3.json  \n",
            "--------------------------------------\n",
            "The length of the training split is: 155\n",
            "--------------------------------------\n",
            "[\n",
            "  {\n",
            "    \"id\": \"08838524-6d11-729c-39d8-fa4d712b7476\",\n",
            "    \"labl\": 1,\n",
            "    \"description\": \"Exhaust steam pipe protector piece fitted below ashpan door Copy drawing EO V2.40 Darlington; B2 Darlington 1943 2 cylinders (B17 rebuild deleted); Darlington; Gorton B1 1948 Classes A2, A3, A4, C2, D1, D2, D3, K2, K3, K4, P2, V2, V4 B1, B2\"\n",
            "  },\n",
            "  {\n",
            "    \"id\": \"467eb98e-1866-708e-03cd-9ab9f8a7f4e5\",\n",
            "    \"labl\": \"oyal Botanic Gardens, Kew\",\n",
            "    \"content\": \"This journal seems to have been kept parallel with RM 1/17 (FKW/1/17), covering Kingdon-Ward's time teaching at the RAF Jungle School up to the end of 1944. In early 1945, not having been offered any work, Kingdon Ward took a trip on the Brahmaputra. In June, he is offered a job on a tea plantation until 30 Nov when he is told to report to Calcutta.\"\n",
            "  },\n",
            "  {\n",
            "    \"id\": \"7ab43067-856c-5cee-8574-ac59a24de44b\",\n",
            "    \"labl\": \"Royal Botanic Gardens, Kew\",\n",
            "    \"text\": null\n",
            "  },\n",
            "  {\n",
            "    \"id\": \"b78f8473-e487-b3cb-be65-4f854d97228b\",\n",
            "    \"labl\": \"Royal Botanic Gardens, Kew\",\n",
            "    \"description\": \"'Remainder Correspondence of the Bibliographical Index' Five letters to and from N D Simpson and C C Kohler, Dorking, Surrey, W Heffer and Sons Ltd, Cambridge, and Wheldon & Wesley Ltd regarding selling the remainder of the Bibliographical Index of which just over 200 are left out of the 750 originally printed. There is also a handwritten note, a postcard and an index card with address.\"\n",
            "  },\n",
            "  {\n",
            "    \"id\": \"ea15bfc9-9499-e9e2-b3e4-654714a795a5\",\n",
            "    \"description\": \"The particulars of an inquest upon the murder of one Thomas Waterman alias Dixon by Lewes Gilberte, butcher, the narrative of which is given as follows,--that Lewis Gilbert, being in the house of Richard Waterman alias Dixon on 22 April, 6 Jac. I., in a low inner parlor, did fall at strife with the said Richard Waterman, his wife and two daughters, who would have put him out of doors, but he set his back against the door and resisted. During the struggle there was an outcry by the daughters, \\\"He will spoile my father, he will murder my father,\\\" whereupon Thomas Waterman, who was sitting with Humphry Acton by the fire in the chimney in the hall right opposite the parlor door, forced it open, and seizing Gilbert by the collar of his doublet, essayed to put him out; but the latter, having drawn a long knife, stabbed him in the right side by the navel, whereof he died the same night. The jurors are extremely particular in minuti\\u00e6 as to the depth and width of the wound, the length and price of the knife, & c\",\n",
            "    \"key\": \"hakespeare Birthplace Trust\"\n",
            "  },\n",
            "  {\n",
            "    \"id\": \"03556d52-3c1e-f36e-97bd-1a09c87ef949\",\n",
            "    \"text\": \"London Brighton & South Coast Railway official photographs. These official company photographs show locomotives under construction at the London, Brighton & South Coast Railway's Brighton Works. (Further references to the Brighton Locomotive Works are to be found in other NRM collections). There are also static views of carriages, engines and tenders at Brighton, Lancing and at Crumbles sidings in Eastbourne, the Royal Train at Battersea and at Epsom Downs for the Derby, Pullman cars used on the 'Southern Belle' service and trials carried out on various locomotives. There are also four negatives of Great Northern locomotives, probably because Douglas Earle Marsh, who was Locomotive Superintendent for the LBSCR for much of the period covered by this collection, had worked under H A Ivatt as Chief Assistant Mechanical Engineer of the GNR. The collection is listed, and there are reference prints in a binder available in the Reading Room.\",\n",
            "    \"label\": \"National Railway Museum\"\n",
            "  },\n",
            "  {\n",
            "    \"id\": \"598e3e51-2390-addf-4363-09503488d4a5\",\n",
            "    \"labl\": 0,\n",
            "    \"text\": \"List of 609 landed properties near Trincomalee with the appraisement and tenure by which it is possessed pre purchase. Contains the names of the proprietors, valuation and remarks relating to tenure.\"\n",
            "  },\n",
            "  {\n",
            "    \"id\": \"f94bc0a8-a0a4-c96f-e3a8-9e71050ff05a\",\n",
            "    \"labl\": \"National Railway Museum\",\n",
            "    \"content\": \"The Sellick collection of photographs featuring railways in Wales, the south of England and the West Country. With particular emphasis on the GWR, Southern and LSWR, together with minor and independent railways. There is coverage, for example, of the Lynton & Barnstaple, Liskeard & Caradon, Oxford & Birmingham, Cambrian, Welshpool & Llanfair, Torrington & Marland Railways, together with LSWR lines into Devon and the railways of the Isle of Wight. Railways in Ireland and France also feature. Prints are available for consultation on request. It can prove difficult to match prints with the appropriate negatives, many of which are copies.\"\n",
            "  },\n",
            "  {\n",
            "    \"id\": \"96355c6a-8bf5-00af-3c97-e297c1967745\",\n",
            "    \"key\": \"National Railway Museum\",\n",
            "    \"content\": \"Euston Public Relations Office, official photographs. These Euston D, DG and DM series negatives are also part of the NRM collections and include images of locomotives, passing trains, places, stations, engine sheds and railway infrastructure. The collection is listed, in chronological order. There are some reference prints, notably views of the demolition of Euston station and its Doric portico. Part listed by the Railprint Joint Venture scheme which marketed these images in the late 1970s and early 1980s\"\n",
            "  },\n",
            "  {\n",
            "    \"id\": \"d676f9c6-54b9-7651-4c8d-c47aa629ed13\",\n",
            "    \"content\": \"This further list details the contents of the double letter series of notation used by the Doncaster Works Drawing Office. It is taken from a register of drawings dated probably about 1935 (Inventory number 2000-7200, original NRM reference number GB 756 2000-7200/AA/A/8/1). It is not known where and for what purposes the list was compiled. It should be noted therefore that as the list has not been checked against the lists of drawings so conclusions, at this time, can not be made as to the accuracy of the list. AA Hydraulic machinery BB Water softeners CC Gas machinery, oil tanks oil pumps etc DD Girders - weighing machine EE Tanks and water cranes FF Plans of buildings - outstations GG Plans of buildings - Doncaster, engines Plans of lines HH Plans of buildings - Doncaster II Furnaces, smiths' hearths, and chimneys JJ Shafting and brackets KK General plans of lathes LL Details of Lathes MM Pumping machinery NN Pumping machinery, including breaking up Sup. OO Portable tolls including air compressors etc PP Steam hammers QQ Saws and saw gearing RR Hydraulic and stretching machines SS Turntables and travellers for engines and Carriages TT Overhead travellers cranes and hoists lifting tackle UU Fixed cranes VV Stationary engines WW Stationary boilers XX Safety valves and stop valves, waterpipes YY Miscellaneous details including shop trollies and water troughs, sanding plants ZZ Line sections and diagrams\",\n",
            "    \"label\": \"National Railway Museum\"\n",
            "  }\n",
            "]\n"
          ]
        }
      ],
      "source": [
        "# Install necessary tools\n",
        "!wget -O dataset.zip https://tinyurl.com/tadarchives\n",
        "!unzip dataset.zip -d dataset\n",
        "import json\n",
        "import pandas as pd\n",
        "\n",
        "# Define the path\n",
        "file_path = \"dataset/dataset.json\"\n",
        "\n",
        "# Load the dataset\n",
        "with open(file_path, \"r\") as f:\n",
        "    museums = json.load(f)\n",
        "\n",
        "# Print  the length and the 10 first examples of the training dataset\n",
        "print('--------------------------------------')\n",
        "print('The length of the training split is: ' + str(len(museums['train'])))\n",
        "\n",
        "print('--------------------------------------')\n",
        "print(json.dumps(museums['train'][:10], indent=2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CeAxBswRtvVa"
      },
      "source": [
        " We can see that there are 3 subcategories for each post. Let's further explore the structure of our training, validation and test datasets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mY_vAASivJ7p",
        "outputId": "ee06330a-3be0-4405-892c-bade9a734a66"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Field distribution in train:\n",
            "{'id': 155, 'labl': 39, 'description': 41, 'content': 56, 'text': 58, 'key': 60, 'label': 56}\n",
            "--------------------------------------------------\n",
            "Field distribution in val:\n",
            "{'id': 50, 'label': 50, 'content': 50}\n",
            "--------------------------------------------------\n",
            "Field distribution in test:\n",
            "{'id': 50, 'label': 50, 'content': 50}\n",
            "--------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "# Function to find all unique keys from the dataset (for training, valitaiton and test)\n",
        "def get_all_keys(dataset):\n",
        "    keys_count = {}\n",
        "    for record in dataset:\n",
        "        for key in record.keys():\n",
        "            keys_count[key] = keys_count.get(key, 0) + 1\n",
        "    return keys_count\n",
        "\n",
        "# Checking unique fields in each dataset split\n",
        "for split in ['train', 'val', 'test']:\n",
        "    print(f\"Field distribution in {split}:\")\n",
        "    print(get_all_keys(museums[split]))  # Count occurrences of each field\n",
        "    print(\"-\" * 50)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C6h-5DJsYXiO"
      },
      "source": [
        "It is clear that the validation and test datasets follow the same structure of 3 subcategories ('id', 'label', 'content'), while the training dataset has its subcategories disorganized. I will further explore my training dataset to ultimately convert it to the relative structure of the validation and training datasets.\n",
        "My next step is to randomly print 2 examples for each of these subcategories for the training dataset, to understand what they represent.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "elSjhzl-X4Vu",
        "outputId": "350504cc-3ae6-4629-bc66-74ff38b68bb7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Random samples for key: id\n",
            "\n",
            "{\n",
            "  \"id\": \"26be998a-5cda-5ff2-a1d2-c48673eb3595\",\n",
            "  \"key\": \"ational Railway Museum\",\n",
            "  \"text\": \"R H and R E Bleasdale collection of railway photographs. Photographs of many companies' locomotives and of Swindon Works in the 1880s appear in two albums of Bleasdale's prints, and in two further albums donated by the Smithsonian Institution. R E Bleasdale negatives are listed.\"\n",
            "}\n",
            "{\n",
            "  \"id\": \"94474c55-5660-4706-63f3-9422a759fda0\",\n",
            "  \"description\": \"Letter from Sir George Talbot to Thomas Blayney in which the former states that he has been advised that the four heriots demanded by the Lady of the Manor at the rate of ten guineas each are too high but that he will agree to pay half of this amount, that is 20 guineas, together with Blayney's bill covering his own expenses.\",\n",
            "  \"labl\": \"hakespeare Birthplace Trust\"\n",
            "}\n",
            "------------------------------------------------------------\n",
            "\n",
            "Random samples for key: labl\n",
            "\n",
            "{\n",
            "  \"id\": \"62e52d58-cb89-4b02-c88f-d3ba8f5d4064\",\n",
            "  \"description\": \"Original drawings of specimens. Original drawings and engravings made from specimens illustrating for the most part the work of Dr. Robert Lee on the nerves of the heart and uterus, drawn by Joseph Perry and Edgar West\",\n",
            "  \"labl\": 3\n",
            "}\n",
            "{\n",
            "  \"id\": \"f94bc0a8-a0a4-c96f-e3a8-9e71050ff05a\",\n",
            "  \"labl\": \"National Railway Museum\",\n",
            "  \"content\": \"The Sellick collection of photographs featuring railways in Wales, the south of England and the West Country. With particular emphasis on the GWR, Southern and LSWR, together with minor and independent railways. There is coverage, for example, of the Lynton & Barnstaple, Liskeard & Caradon, Oxford & Birmingham, Cambrian, Welshpool & Llanfair, Torrington & Marland Railways, together with LSWR lines into Devon and the railways of the Isle of Wight. Railways in Ireland and France also feature. Prints are available for consultation on request. It can prove difficult to match prints with the appropriate negatives, many of which are copies.\"\n",
            "}\n",
            "------------------------------------------------------------\n",
            "\n",
            "Random samples for key: description\n",
            "\n",
            "{\n",
            "  \"id\": \"5911d963-3dae-5839-2443-960930f73aa3\",\n",
            "  \"label\": \"National Maritime Museum\",\n",
            "  \"description\": \"Deposition by Thomas Jewell, Shipwright. When coming to work at 6.30, he saw William Steward, Shipwright, by the dock wall near the house of Sir John Hamilton, who picked up a white line which he said would make a clothes line. Steward then turned back and met George Wadley, who joined Jewell and informed him that the line had knots in it and some tow and matches were with it. Witnessed by Samuel Hood. Enclosure.\"\n",
            "}\n",
            "{\n",
            "  \"id\": \"7bdae410-a8f0-db6c-3780-7beba41262d5\",\n",
            "  \"label\": \"ational Railway Museum\",\n",
            "  \"description\": \"Letters and papers of C Grasemann, amassed during his time with the Southern Railway. Also includes photographs of notable events such as royal visits and an escaped lion at Clapham Junction. Contemporaries record that Grasemann possessed a formidable personality but also had a strong sense of humour, and the NRM archive contains a book of humorous pen portraits of his acquaintances. He had a passion for yachting and his collection comprises an album of yachting scenes. All the photographs exist as prints only.\"\n",
            "}\n",
            "------------------------------------------------------------\n",
            "\n",
            "Random samples for key: content\n",
            "\n",
            "{\n",
            "  \"id\": \"59287772-bb77-05fe-9e17-5048ff184ae0\",\n",
            "  \"content\": \"Manuscript of Memoranda Botanica. Volume 1. This volume is made up of three parts: Part one is the Ephemeris Botanica which includes detailed descriptions of plants, written in Latin; it includes some sketches and ranges from 15 December 1800 to 24 September 1823. The title of part two is 'Memorandum relating to Herbaria of Portugal, Madeira, Tenerife and Brazil' and it documents the labelling and packaging details following the collection of the plants. There is a description which says, 'The Brazilian Herbarium (including those of Portugal, Madeira and Tenerife) collected between 25 March 1825 (arrival at Lisbon) and 10 February 1830 (sailing from Par\\u00e1) was begun to be unpacked at Fulham on 3 February 1847 and ended on 15 February 1850'. There are pages of data in tabulated form under the headings: parcel or package number; number of the Catal. Geog; country or place where collected; date when each parcel was unpacked. 133 packages are included in this table. The next list describes the 'state in which the specimens were found to be at the time when they were unpacked at Fulham from Feb 1847 to Feb 1850'. Following this is 'An account of the quantity of paper finally used after unpacking the Brazilian Herbarium from Feb 1847 to Feb 1850'. There is then an account of the system used to affix labels to the specimens in the Brazilian Herbarium. Another table lists the specimens which are missing and further list shows the number of specimens counted in the Herbarium. Part three of this volume is called 'A Catalogue of Bulbous Roots brought from Southern Africa'. The list shows how many bulbs he had of each variety from 12 March 1816 and shows when they flowered. It continues to 10 October 1826. There is a table showing 'Bulbs in the borders as they stand in succession' and finally a list documenting the bulbs which were given away, showing the date, name of the recipient and the bulb number.\",\n",
            "  \"key\": 2\n",
            "}\n",
            "{\n",
            "  \"id\": \"aa985a46-605f-2be7-12ba-9204d8d240e4\",\n",
            "  \"labl\": \"Shakespeare Birthplace Trust\",\n",
            "  \"content\": 829308\n",
            "}\n",
            "------------------------------------------------------------\n",
            "\n",
            "Random samples for key: text\n",
            "\n",
            "{\n",
            "  \"id\": \"2dcbf6be-60a4-2a0f-d83a-d01f68755a4d\",\n",
            "  \"text\": \"Declaration of Dr. George Macaulay, Licentiate, that the Defendant George Edwards was on the 1st December 1752 indebted to him in \\u00a3200 for so much money before that time received, and that the Defendant took upon himself to repay the said \\u00a3200 to Plaintiff when required, but that he refused to do so. NOTE: This is one of the actions in which the Licentiates of the College, prior to the famous test case of Archer & Fothergill, q.v. and co-existent with the cases of other Licentiates Andr\\u00e9e, Clephane, Cox, Castro-Sarmento, Ross & Silvester, q.v., endeavoured to obtain redress of their supposed grievances in being refused admission to the Fellowship. The action is brought against the Beadle (who, according to the Declaration, is in the custody of the Marshal of the Marshalsea) for the recovery of the sum of \\u00a330, the fee for the Plaintiffs admission as a Licentiate, and other sums to the Officers of the College as set out hereunder; also the annual sum of \\u00a32 paid to the College as a Licentiate. To the College \\u00a330- 0s- 0d. To the President 1- 0- 0. To the Censors 2- 0- 0. To the Treasurer 6s- 8d. To the Register 11- 8. Beadle 3- 4. Diploma 10- 0. Stamps on the Diploma 6- 0- 0. Stamps on the Register 2- 0. A Bond to be given 10- 0. 41- 3- 8.\",\n",
            "  \"label\": \"Royal College of Physicians of London\"\n",
            "}\n",
            "{\n",
            "  \"id\": \"c318fb75-5395-cef2-9d0e-b6f1828d640f\",\n",
            "  \"text\": \"Photographs illustrating reports studying the effects of wear on track, tyres and springs. Mainly microphotographs or cross sections showing the metallurgical structure of rails, accompanied by detailed descriptions. The reports are available for consultation on request. There are no original negatives in the NRM collections.\",\n",
            "  \"key\": \"ational Railway Museum\"\n",
            "}\n",
            "------------------------------------------------------------\n",
            "\n",
            "Random samples for key: key\n",
            "\n",
            "{\n",
            "  \"id\": \"eda86546-5f67-cca0-b033-45b5e1c9002b\",\n",
            "  \"key\": \"hakespeare Birthplace Trust\",\n",
            "  \"description\": \"The entry of the summons in the same suit, by which it appears that at the instance of William Hobday, the plaintiff lent one William Badger, being about to go to London, a horse to ride there, defendant engaging that he should be no loser, that the horse should be well used and redelivered safe and sound at Stratford. The conditions of the promise have not been complied with, and the damages are laid at four pounds, 40 Eliz\"\n",
            "}\n",
            "{\n",
            "  \"id\": \"50145e24-0917-9434-f461-a42b0c1ce044\",\n",
            "  \"key\": \"oyal Botanic Gardens, Kew\",\n",
            "  \"description\": \"Inventory of the Botanical Collection of Books. This volume is the official copy of the Inventory. The collection was purchased by the Commissioners of Her Majesty's Works and Public Buildings; his copy was to be placed in the Library.\"\n",
            "}\n",
            "------------------------------------------------------------\n",
            "\n",
            "Random samples for key: label\n",
            "\n",
            "{\n",
            "  \"id\": \"1569acd0-030c-7a99-870b-f3d518b52559\",\n",
            "  \"label\": \"oyal Botanic Gardens, Kew\",\n",
            "  \"content\": \"Letter from Charles Darwin, Shrewsbury, to John Stevens Henslow. Darwin explains how he feels he should decline the 'Beagle' voyage offer from Henslow because of his father\\u2019s objections, which he lists, and that otherwise he would have taken all risks to go on it. His Geological trip with Adam Sedgwick was a success and he has been grieving from the news of Marmaduke Ramsay\\u2019s death.\"\n",
            "}\n",
            "{\n",
            "  \"id\": \"48efca0b-cd73-9f09-3794-8def93afa8aa\",\n",
            "  \"content\": \"BALDWIN (Francis John Augustus) d. 1915. Correspondence between Dr. Liveing and the Editors of the Medical Directory re right of Francis John Augustus Baldwin, Frederic William Martin and George Trevor Harley Thomas to style themselves LRCP.\",\n",
            "  \"label\": \"oyal College of Physicians of London\"\n",
            "}\n",
            "------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "\n",
        "# Function to sample random records for each key in train dataset\n",
        "def sample_train_records(dataset, num_samples=2):\n",
        "    for key in [\"id\", \"labl\", \"description\", \"content\", \"text\", \"key\", \"label\"]:\n",
        "        print(f\"\\nRandom samples for key: {key}\\n\")\n",
        "\n",
        "        # Filter records that contain the specific key\n",
        "        matching_records = [record for record in dataset if key in record and record[key]]\n",
        "\n",
        "        # Randomly sample from matching records\n",
        "        if matching_records:\n",
        "            samples = random.sample(matching_records, min(num_samples, len(matching_records)))\n",
        "            for sample in samples:\n",
        "                print(json.dumps(sample, indent=2))\n",
        "        else:\n",
        "            print(f\"No records found for key: {key}\")\n",
        "\n",
        "        print(\"-\" * 60)\n",
        "\n",
        "# Run sampling function on the train dataset\n",
        "sample_train_records(museums[\"train\"])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E1xNliEjbRhG"
      },
      "source": [
        "It is clear that 'label', 'labl' and 'key' represent the same thing, and I will convert all these to 'label'. The same applies for 'content', 'description' and 'text', which I will convert to 'content'. Also, some of the examples indicate that the subcategories don't follow the same sequence always, which should be ('id' > 'label' > 'content'). I will take care of this as well."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qhxSXQSPckXR",
        "outputId": "d0c87227-de9e-41ec-8c60-a91d000f66a7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Field distribution in train split:\n",
            "{'id': 155, 'label': 155, 'content': 155}\n",
            "--------------------------------------------------\n",
            "[\n",
            "  {\n",
            "    \"id\": \"08838524-6d11-729c-39d8-fa4d712b7476\",\n",
            "    \"label\": 1,\n",
            "    \"content\": \"Exhaust steam pipe protector piece fitted below ashpan door Copy drawing EO V2.40 Darlington; B2 Darlington 1943 2 cylinders (B17 rebuild deleted); Darlington; Gorton B1 1948 Classes A2, A3, A4, C2, D1, D2, D3, K2, K3, K4, P2, V2, V4 B1, B2\"\n",
            "  },\n",
            "  {\n",
            "    \"id\": \"467eb98e-1866-708e-03cd-9ab9f8a7f4e5\",\n",
            "    \"label\": \"oyal Botanic Gardens, Kew\",\n",
            "    \"content\": \"This journal seems to have been kept parallel with RM 1/17 (FKW/1/17), covering Kingdon-Ward's time teaching at the RAF Jungle School up to the end of 1944. In early 1945, not having been offered any work, Kingdon Ward took a trip on the Brahmaputra. In June, he is offered a job on a tea plantation until 30 Nov when he is told to report to Calcutta.\"\n",
            "  },\n",
            "  {\n",
            "    \"id\": \"7ab43067-856c-5cee-8574-ac59a24de44b\",\n",
            "    \"label\": \"Royal Botanic Gardens, Kew\",\n",
            "    \"content\": null\n",
            "  },\n",
            "  {\n",
            "    \"id\": \"b78f8473-e487-b3cb-be65-4f854d97228b\",\n",
            "    \"label\": \"Royal Botanic Gardens, Kew\",\n",
            "    \"content\": \"'Remainder Correspondence of the Bibliographical Index' Five letters to and from N D Simpson and C C Kohler, Dorking, Surrey, W Heffer and Sons Ltd, Cambridge, and Wheldon & Wesley Ltd regarding selling the remainder of the Bibliographical Index of which just over 200 are left out of the 750 originally printed. There is also a handwritten note, a postcard and an index card with address.\"\n",
            "  },\n",
            "  {\n",
            "    \"id\": \"ea15bfc9-9499-e9e2-b3e4-654714a795a5\",\n",
            "    \"label\": \"hakespeare Birthplace Trust\",\n",
            "    \"content\": \"The particulars of an inquest upon the murder of one Thomas Waterman alias Dixon by Lewes Gilberte, butcher, the narrative of which is given as follows,--that Lewis Gilbert, being in the house of Richard Waterman alias Dixon on 22 April, 6 Jac. I., in a low inner parlor, did fall at strife with the said Richard Waterman, his wife and two daughters, who would have put him out of doors, but he set his back against the door and resisted. During the struggle there was an outcry by the daughters, \\\"He will spoile my father, he will murder my father,\\\" whereupon Thomas Waterman, who was sitting with Humphry Acton by the fire in the chimney in the hall right opposite the parlor door, forced it open, and seizing Gilbert by the collar of his doublet, essayed to put him out; but the latter, having drawn a long knife, stabbed him in the right side by the navel, whereof he died the same night. The jurors are extremely particular in minuti\\u00e6 as to the depth and width of the wound, the length and price of the knife, & c\"\n",
            "  }\n",
            "]\n"
          ]
        }
      ],
      "source": [
        "# Function to standardize column names and order\n",
        "def standardize_records(dataset):\n",
        "    standardized_data = []\n",
        "\n",
        "    for record in dataset:\n",
        "        standardized_record = {}\n",
        "\n",
        "        # Keep the existing ID\n",
        "        standardized_record[\"id\"] = record.get(\"id\", None)\n",
        "\n",
        "        # Extract label (preserve existing valid labels)\n",
        "        if \"label\" in record and record[\"label\"] is not None:\n",
        "            standardized_record[\"label\"] = record[\"label\"]\n",
        "        elif \"labl\" in record and record[\"labl\"] is not None:\n",
        "            standardized_record[\"label\"] = record[\"labl\"]\n",
        "        elif \"key\" in record and record[\"key\"] is not None:\n",
        "            standardized_record[\"label\"] = record[\"key\"]\n",
        "        else:\n",
        "            standardized_record[\"label\"] = None  # Only set None if no label exists\n",
        "\n",
        "        # Extract content\n",
        "        standardized_record[\"content\"] = (\n",
        "            record.get(\"content\") or record.get(\"description\") or record.get(\"text\") or None\n",
        "        )\n",
        "\n",
        "        standardized_data.append(standardized_record)\n",
        "\n",
        "    return standardized_data\n",
        "\n",
        "\n",
        "# Apply the standardization to the training split\n",
        "museums[\"train\"] = standardize_records(museums[\"train\"])\n",
        "\n",
        "# Check a sample after standardization\n",
        "print(f\"Field distribution in train split:\")\n",
        "print(get_all_keys(museums[\"train\"]))  # Count occurrences of each field\n",
        "print(\"-\" * 50)\n",
        "print(json.dumps(museums[\"train\"][:5], indent=2))  # Print first 3 records of train set\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mbie-JuwfWQL"
      },
      "source": [
        "There are some missing letters in some of the labels, so I am going to fix this using TF-IDF and Cosine Similarity.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dJ8mmUvbfhPz",
        "outputId": "4c3b3944-42aa-4c87-b6b6-059d2fca9e3a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[\n",
            "  {\n",
            "    \"id\": \"08838524-6d11-729c-39d8-fa4d712b7476\",\n",
            "    \"label\": 1,\n",
            "    \"content\": \"Exhaust steam pipe protector piece fitted below ashpan door Copy drawing EO V2.40 Darlington; B2 Darlington 1943 2 cylinders (B17 rebuild deleted); Darlington; Gorton B1 1948 Classes A2, A3, A4, C2, D1, D2, D3, K2, K3, K4, P2, V2, V4 B1, B2\"\n",
            "  },\n",
            "  {\n",
            "    \"id\": \"467eb98e-1866-708e-03cd-9ab9f8a7f4e5\",\n",
            "    \"label\": \"Royal Botanic Gardens, Kew\",\n",
            "    \"content\": \"This journal seems to have been kept parallel with RM 1/17 (FKW/1/17), covering Kingdon-Ward's time teaching at the RAF Jungle School up to the end of 1944. In early 1945, not having been offered any work, Kingdon Ward took a trip on the Brahmaputra. In June, he is offered a job on a tea plantation until 30 Nov when he is told to report to Calcutta.\"\n",
            "  },\n",
            "  {\n",
            "    \"id\": \"7ab43067-856c-5cee-8574-ac59a24de44b\",\n",
            "    \"label\": \"Royal Botanic Gardens, Kew\",\n",
            "    \"content\": null\n",
            "  },\n",
            "  {\n",
            "    \"id\": \"b78f8473-e487-b3cb-be65-4f854d97228b\",\n",
            "    \"label\": \"Royal Botanic Gardens, Kew\",\n",
            "    \"content\": \"'Remainder Correspondence of the Bibliographical Index' Five letters to and from N D Simpson and C C Kohler, Dorking, Surrey, W Heffer and Sons Ltd, Cambridge, and Wheldon & Wesley Ltd regarding selling the remainder of the Bibliographical Index of which just over 200 are left out of the 750 originally printed. There is also a handwritten note, a postcard and an index card with address.\"\n",
            "  },\n",
            "  {\n",
            "    \"id\": \"ea15bfc9-9499-e9e2-b3e4-654714a795a5\",\n",
            "    \"label\": \"Shakespeare Birthplace Trust\",\n",
            "    \"content\": \"The particulars of an inquest upon the murder of one Thomas Waterman alias Dixon by Lewes Gilberte, butcher, the narrative of which is given as follows,--that Lewis Gilbert, being in the house of Richard Waterman alias Dixon on 22 April, 6 Jac. I., in a low inner parlor, did fall at strife with the said Richard Waterman, his wife and two daughters, who would have put him out of doors, but he set his back against the door and resisted. During the struggle there was an outcry by the daughters, \\\"He will spoile my father, he will murder my father,\\\" whereupon Thomas Waterman, who was sitting with Humphry Acton by the fire in the chimney in the hall right opposite the parlor door, forced it open, and seizing Gilbert by the collar of his doublet, essayed to put him out; but the latter, having drawn a long knife, stabbed him in the right side by the navel, whereof he died the same night. The jurors are extremely particular in minuti\\u00e6 as to the depth and width of the wound, the length and price of the knife, & c\"\n",
            "  }\n",
            "]\n"
          ]
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "\n",
        "# Define correct labels\n",
        "correct_labels = [\n",
        "    \"National Maritime Museum\",\n",
        "    \"National Railway Museum\",\n",
        "    \"Royal Botanic Gardens, Kew\",\n",
        "    \"Royal College of Physicians of London\",\n",
        "    \"Shakespeare Birthplace Trust\"\n",
        "]\n",
        "\n",
        "# Vectorize correct labels\n",
        "vectorizer = TfidfVectorizer()\n",
        "correct_label_vectors = vectorizer.fit_transform(correct_labels)\n",
        "\n",
        "# Function to fix labels using TF-IDF + Cosine Similarity\n",
        "def fix_labels_tfidf(dataset):\n",
        "    for record in dataset:\n",
        "        if record[\"label\"] and isinstance(record[\"label\"], str):\n",
        "            # Transform the corrupted label into a vector\n",
        "            label_vector = vectorizer.transform([record[\"label\"]])\n",
        "\n",
        "            # Compute cosine similarity with correct labels\n",
        "            similarities = cosine_similarity(label_vector, correct_label_vectors)\n",
        "\n",
        "            # Get the best match\n",
        "            best_match_index = np.argmax(similarities)\n",
        "            best_match = correct_labels[best_match_index]\n",
        "            confidence = similarities[0, best_match_index]\n",
        "\n",
        "            # Replace if confidence is high\n",
        "            if confidence > 0.80:\n",
        "                record[\"label\"] = best_match\n",
        "            else:\n",
        "                print(f\"\u26a0 Warning: Low confidence fix for '{record['label']}' -> '{best_match}' (Score: {confidence:.2f})\")\n",
        "\n",
        "    return dataset\n",
        "\n",
        "# Apply label fixing\n",
        "museums[\"train\"] = fix_labels_tfidf(museums[\"train\"])\n",
        "# museums[\"val\"] = fix_labels_tfidf(museums[\"val\"])\n",
        "# museums[\"test\"] = fix_labels_tfidf(museums[\"test\"])\n",
        "\n",
        "# Print some corrected labels\n",
        "print(json.dumps(museums[\"train\"][:5], indent=2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TusuVJM6iJOt"
      },
      "source": [
        "It seems that this problem is solved. Now, I am going to check if there are any missing values in my train split, before proceeding with the analysis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nvlpS4GMiePB",
        "outputId": "547f3fd5-530b-4893-c692-30e16dead28e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Missing Values in Training Dataset:\n",
            "Missing ID: 0\n",
            "Missing Label: 0\n",
            "Missing Content: 2\n"
          ]
        }
      ],
      "source": [
        "# Count missing values in the training dataset\n",
        "missing_id = sum(1 for record in museums[\"train\"] if record.get(\"id\") is None)\n",
        "missing_label = sum(1 for record in museums[\"train\"] if record.get(\"label\") is None)\n",
        "missing_content = sum(1 for record in museums[\"train\"] if record.get(\"content\") is None)\n",
        "\n",
        "# Print missing value counts\n",
        "print(\"Missing Values in Training Dataset:\")\n",
        "print(f\"Missing ID: {missing_id}\")\n",
        "print(f\"Missing Label: {missing_label}\")\n",
        "print(f\"Missing Content: {missing_content}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q9lVWiqjoGt9"
      },
      "source": [
        "Now I will check for any inconsistencies in the 'content' subcategory and fix them if necessary."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bdy9DceIoXtf",
        "outputId": "12a97b71-862a-413b-8f9b-effd58bc0dad"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Total Missing (None) `content`: 2\n",
            "{\n",
            "  \"id\": \"7ab43067-856c-5cee-8574-ac59a24de44b\",\n",
            "  \"label\": \"Royal Botanic Gardens, Kew\",\n",
            "  \"content\": null\n",
            "}\n",
            "------------------------------------------------------------\n",
            "{\n",
            "  \"id\": \"95a0d569-602a-5bdc-06ca-62eb08302d3d\",\n",
            "  \"label\": \"National Railway Museum\",\n",
            "  \"content\": null\n",
            "}\n",
            "------------------------------------------------------------\n",
            "\n",
            "Removed 2 records with missing content.\n",
            "\n",
            "\u26a0 Total Invalid (Non-String) `content`: 3\n",
            "{\n",
            "  \"id\": \"7148fce2-2315-1826-2615-05abd9b0b806\",\n",
            "  \"label\": \"National Railway Museum\",\n",
            "  \"content\": 258157\n",
            "}\n",
            "------------------------------------------------------------\n",
            "{\n",
            "  \"id\": \"aa985a46-605f-2be7-12ba-9204d8d240e4\",\n",
            "  \"label\": \"Shakespeare Birthplace Trust\",\n",
            "  \"content\": 829308\n",
            "}\n",
            "------------------------------------------------------------\n",
            "{\n",
            "  \"id\": \"246aeb01-6686-57ab-9f4e-3a15f01446e5\",\n",
            "  \"label\": \"Royal College of Physicians of London\",\n",
            "  \"content\": 535970\n",
            "}\n",
            "------------------------------------------------------------\n",
            "\n",
            "Removed 3 records with invalid content.\n"
          ]
        }
      ],
      "source": [
        "# Print and Remove Missing (None) `content`\n",
        "def remove_missing_content(dataset):\n",
        "    missing_records = [record for record in dataset if record.get(\"content\") is None]\n",
        "\n",
        "    # Print number of missing records\n",
        "    print(f\"\\nTotal Missing (None) `content`: {len(missing_records)}\")\n",
        "\n",
        "    # Print records before deleting\n",
        "    for record in missing_records:\n",
        "        print(json.dumps(record, indent=2))\n",
        "        print(\"-\" * 60)\n",
        "\n",
        "    # Remove records with missing content\n",
        "    cleaned_dataset = [record for record in dataset if record.get(\"content\") is not None]\n",
        "    print(f\"\\nRemoved {len(missing_records)} records with missing content.\")\n",
        "\n",
        "    return cleaned_dataset\n",
        "\n",
        "# Print and Remove Invalid `content` (not a string)\n",
        "def remove_invalid_content(dataset):\n",
        "    invalid_records = [record for record in dataset if not isinstance(record.get(\"content\"), str)]\n",
        "\n",
        "    # Print number of invalid records\n",
        "    print(f\"\\n\u26a0 Total Invalid (Non-String) `content`: {len(invalid_records)}\")\n",
        "\n",
        "    # Print records before deleting\n",
        "    for record in invalid_records:\n",
        "        print(json.dumps(record, indent=2))\n",
        "        print(\"-\" * 60)\n",
        "\n",
        "    # Remove records with invalid content\n",
        "    cleaned_dataset = [record for record in dataset if isinstance(record.get(\"content\"), str)]\n",
        "    print(f\"\\nRemoved {len(invalid_records)} records with invalid content.\")\n",
        "\n",
        "    return cleaned_dataset\n",
        "\n",
        "# Apply Both Steps to Training Data\n",
        "museums[\"train\"] = remove_missing_content(museums[\"train\"])  # Remove missing content\n",
        "museums[\"train\"] = remove_invalid_content(museums[\"train\"])  # Remove invalid content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DVzZ1lCHzxPE"
      },
      "source": [
        "Now I am going to convert my labels to their corresponding indices. I will do this for all the splits.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hlsXRPhx0KEH",
        "outputId": "514c9c9e-a344-4d47-9802-2526b3c34ecc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\ud83d\udcc4 Sample of Training Data with Converted Labels:\n",
            "[\n",
            "  {\n",
            "    \"id\": \"08838524-6d11-729c-39d8-fa4d712b7476\",\n",
            "    \"label\": 1,\n",
            "    \"content\": \"Exhaust steam pipe protector piece fitted below ashpan door Copy drawing EO V2.40 Darlington; B2 Darlington 1943 2 cylinders (B17 rebuild deleted); Darlington; Gorton B1 1948 Classes A2, A3, A4, C2, D1, D2, D3, K2, K3, K4, P2, V2, V4 B1, B2\"\n",
            "  },\n",
            "  {\n",
            "    \"id\": \"467eb98e-1866-708e-03cd-9ab9f8a7f4e5\",\n",
            "    \"label\": 2,\n",
            "    \"content\": \"This journal seems to have been kept parallel with RM 1/17 (FKW/1/17), covering Kingdon-Ward's time teaching at the RAF Jungle School up to the end of 1944. In early 1945, not having been offered any work, Kingdon Ward took a trip on the Brahmaputra. In June, he is offered a job on a tea plantation until 30 Nov when he is told to report to Calcutta.\"\n",
            "  },\n",
            "  {\n",
            "    \"id\": \"b78f8473-e487-b3cb-be65-4f854d97228b\",\n",
            "    \"label\": 2,\n",
            "    \"content\": \"'Remainder Correspondence of the Bibliographical Index' Five letters to and from N D Simpson and C C Kohler, Dorking, Surrey, W Heffer and Sons Ltd, Cambridge, and Wheldon & Wesley Ltd regarding selling the remainder of the Bibliographical Index of which just over 200 are left out of the 750 originally printed. There is also a handwritten note, a postcard and an index card with address.\"\n",
            "  },\n",
            "  {\n",
            "    \"id\": \"ea15bfc9-9499-e9e2-b3e4-654714a795a5\",\n",
            "    \"label\": 4,\n",
            "    \"content\": \"The particulars of an inquest upon the murder of one Thomas Waterman alias Dixon by Lewes Gilberte, butcher, the narrative of which is given as follows,--that Lewis Gilbert, being in the house of Richard Waterman alias Dixon on 22 April, 6 Jac. I., in a low inner parlor, did fall at strife with the said Richard Waterman, his wife and two daughters, who would have put him out of doors, but he set his back against the door and resisted. During the struggle there was an outcry by the daughters, \\\"He will spoile my father, he will murder my father,\\\" whereupon Thomas Waterman, who was sitting with Humphry Acton by the fire in the chimney in the hall right opposite the parlor door, forced it open, and seizing Gilbert by the collar of his doublet, essayed to put him out; but the latter, having drawn a long knife, stabbed him in the right side by the navel, whereof he died the same night. The jurors are extremely particular in minuti\\u00e6 as to the depth and width of the wound, the length and price of the knife, & c\"\n",
            "  },\n",
            "  {\n",
            "    \"id\": \"03556d52-3c1e-f36e-97bd-1a09c87ef949\",\n",
            "    \"label\": 1,\n",
            "    \"content\": \"London Brighton & South Coast Railway official photographs. These official company photographs show locomotives under construction at the London, Brighton & South Coast Railway's Brighton Works. (Further references to the Brighton Locomotive Works are to be found in other NRM collections). There are also static views of carriages, engines and tenders at Brighton, Lancing and at Crumbles sidings in Eastbourne, the Royal Train at Battersea and at Epsom Downs for the Derby, Pullman cars used on the 'Southern Belle' service and trials carried out on various locomotives. There are also four negatives of Great Northern locomotives, probably because Douglas Earle Marsh, who was Locomotive Superintendent for the LBSCR for much of the period covered by this collection, had worked under H A Ivatt as Chief Assistant Mechanical Engineer of the GNR. The collection is listed, and there are reference prints in a binder available in the Reading Room.\"\n",
            "  }\n",
            "]\n",
            "Missing Values in Training Dataset:\n",
            "Missing ID: 0\n",
            "Missing Label: 0\n",
            "Missing Content: 0\n",
            "--------------------------------------\n",
            "The length of the training split is: 150\n"
          ]
        }
      ],
      "source": [
        "# Define the Label-to-Index Mapping\n",
        "label_mapping = {\n",
        "    \"National Maritime Museum\": 0,\n",
        "    \"National Railway Museum\": 1,\n",
        "    \"Royal Botanic Gardens, Kew\": 2,\n",
        "    \"Royal College of Physicians of London\": 3,\n",
        "    \"Shakespeare Birthplace Trust\": 4\n",
        "}\n",
        "\n",
        "# Convert String Labels to Integer Indices\n",
        "def convert_labels_to_indices(dataset):\n",
        "    for record in dataset:\n",
        "        if isinstance(record[\"label\"], str) and record[\"label\"] in label_mapping:\n",
        "            record[\"label\"] = label_mapping[record[\"label\"]]\n",
        "        elif record[\"label\"] is None:\n",
        "            print(f\"\u26a0 Warning: Missing label for record ID {record['id']}\")\n",
        "\n",
        "    return dataset\n",
        "\n",
        "# Apply the conversion to all datasets\n",
        "museums[\"train\"] = convert_labels_to_indices(museums[\"train\"])\n",
        "museums[\"val\"] = convert_labels_to_indices(museums[\"val\"])\n",
        "museums[\"test\"] = convert_labels_to_indices(museums[\"test\"])\n",
        "\n",
        "# Print a few converted records\n",
        "print(\"\\n\ud83d\udcc4 Sample of Training Data with Converted Labels:\")\n",
        "print(json.dumps(museums[\"train\"][:5], indent=2))\n",
        "\n",
        "# Count missing values in the training dataset\n",
        "missing_id = sum(1 for record in museums[\"train\"] if record.get(\"id\") is None)\n",
        "missing_label = sum(1 for record in museums[\"train\"] if record.get(\"label\") is None)\n",
        "missing_content = sum(1 for record in museums[\"train\"] if record.get(\"content\") is None)\n",
        "\n",
        "# Print missing value counts\n",
        "print(\"Missing Values in Training Dataset:\")\n",
        "print(f\"Missing ID: {missing_id}\")\n",
        "print(f\"Missing Label: {missing_label}\")\n",
        "print(f\"Missing Content: {missing_content}\")\n",
        "print('--------------------------------------')\n",
        "print('The length of the training split is: ' + str(len(museums['train'])))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bZXqYUJX44kW"
      },
      "source": [
        "We can see that now we have no missing values, and our train split is ready for use."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BkNfSNknLrze"
      },
      "source": [
        "##Exploration##\n",
        "Once the training set has been fixed, report the following:\n",
        "*  The sample counts for the training, validation and test sets\n",
        "*    The percentage splits for training, validation and test sets\n",
        "*   The minimum and maximum length (in characters) of the texts. Report separately for the training, validation\n",
        "and test sets\n",
        "*   The most frequent five tokens in each class (after tokenizing with *text_pipeline_spacy* from Lab 2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0dxwPgncH8Ut",
        "outputId": "276d0532-1d2d-40ea-85d5-a8a550427a88"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training samples: 150\n",
            "Validation samples: 50\n",
            "Test samples: 50\n",
            "Training: 60.00%\n",
            "Validation: 20.00%\n",
            "Test: 20.00%\n",
            "Train Text Lengths: Min=163, Max=4263\n",
            "Validation Text Lengths: Min=154, Max=2794\n",
            "Test Text Lengths: Min=167, Max=3479\n",
            "Class 0: [('john', 16), ('sir', 11), ('henry', 10), ('enclosure', 10), ('james', 9)]\n",
            "Class 1: [('2000', 144), ('7200', 144), ('756', 143), ('gb', 142), ('collection', 31)]\n",
            "Class 2: [('letter', 30), ('include', 15), ('paper', 14), ('list', 12), ('contain', 11)]\n",
            "Class 3: [('seal', 25), ('common', 21), ('mr.', 17), ('college', 17), ('fellow', 16)]\n",
            "Class 4: [('mr.', 18), ('work', 16), ('bill', 15), ('account', 14), ('letter', 14)]\n"
          ]
        }
      ],
      "source": [
        "from collections import Counter\n",
        "import spacy\n",
        "\n",
        "# Extract dataset splits\n",
        "train_set = museums['train']\n",
        "val_set = museums['val']\n",
        "test_set = museums['test']\n",
        "\n",
        "# Sample counts per dataset\n",
        "train_count = len(train_set)\n",
        "val_count = len(val_set)\n",
        "test_count = len(test_set)\n",
        "total_count = train_count + val_count + test_count\n",
        "\n",
        "print(f\"Training samples: {train_count}\")\n",
        "print(f\"Validation samples: {val_count}\")\n",
        "print(f\"Test samples: {test_count}\")\n",
        "\n",
        "# Percentage splits per dataset\n",
        "train_pct = (train_count / total_count) * 100\n",
        "val_pct = (val_count / total_count) * 100\n",
        "test_pct = (test_count / total_count) * 100\n",
        "\n",
        "print(f\"Training: {train_pct:.2f}%\")\n",
        "print(f\"Validation: {val_pct:.2f}%\")\n",
        "print(f\"Test: {test_pct:.2f}%\")\n",
        "\n",
        "# Compute min/max text lengths per dataset\n",
        "def text_lengths(dataset, key='content'):\n",
        "    lengths = [len(item[key]) for item in dataset if key in item]\n",
        "    return min(lengths), max(lengths)\n",
        "\n",
        "train_min, train_max = text_lengths(train_set)\n",
        "val_min, val_max = text_lengths(val_set)\n",
        "test_min, test_max = text_lengths(test_set)\n",
        "\n",
        "print(f\"Train Text Lengths: Min={train_min}, Max={train_max}\")\n",
        "print(f\"Validation Text Lengths: Min={val_min}, Max={val_max}\")\n",
        "print(f\"Test Text Lengths: Min={test_min}, Max={test_max}\")\n",
        "\n",
        "# Load spaCy for tokenization\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Use text_pipeline_spacy from Lab 2\n",
        "def text_pipeline_spacy(text):\n",
        "    tokens = []\n",
        "    doc = nlp(text)\n",
        "    for t in doc:\n",
        "        if not t.is_stop and not t.is_punct and not t.is_space:\n",
        "            tokens.append(t.lemma_.lower())\n",
        "    return tokens\n",
        "\n",
        "# Get most frequent tokens per class\n",
        "class_counts = {i: Counter() for i in range(5)}\n",
        "for item in train_set:\n",
        "    label = item['label']\n",
        "    tokens = text_pipeline_spacy(item['content'])  # Using lab function now\n",
        "    class_counts[label].update(tokens)\n",
        "\n",
        "# Display top 5 tokens per class\n",
        "for label, counter in class_counts.items():\n",
        "    print(f\"Class {label}: {counter.most_common(5)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ep5gMgUty42J"
      },
      "source": [
        "##Prompting with a large language model\n",
        "Calculate the accuracy, macro precision, macro recall and macro F1 for each prompt template and comment on the result.\n",
        "Consider any invalid output from the LLM as predicting a sixth hypothetical class."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pw7hHjdyzDwt",
        "outputId": "a6602aee-77e4-4742-801f-038fc7c7ac15"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Prompt Template  Accuracy  Macro Precision  Macro Recall  Macro F1\n",
            "0        Prompt 1  0.000000         0.000000      0.000000  0.000000\n",
            "1        Prompt 2  0.760000         0.788615      0.754925  0.743436\n",
            "2        Prompt 3  0.706667         0.674268      0.582762  0.584574\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# Ensure the dataset is correctly loaded\n",
        "def get_ground_truth_labels(museums):\n",
        "    return [record[\"label\"] for record in museums[\"train\"]]\n",
        "\n",
        "# Function to load predictions safely\n",
        "def load_predictions(file_path):\n",
        "    if not os.path.exists(file_path):\n",
        "        print(f\"\u26a0 Warning: File not found: {file_path}. Skipping this prompt.\")\n",
        "        return None\n",
        "    with open(file_path, \"r\") as f:\n",
        "        predictions_data = json.load(f)\n",
        "    return [entry.get(\"next_token\") for entry in predictions_data]\n",
        "\n",
        "# Paths to the uploaded prediction files\n",
        "prompt_files = {\n",
        "    \"Prompt 1\": \"dataset/llm_prompt_template_1.json\",\n",
        "    \"Prompt 2\": \"dataset/llm_prompt_template_2.json\",\n",
        "    \"Prompt 3\": \"dataset/llm_prompt_template_3.json\"\n",
        "}\n",
        "\n",
        "# Load ground truth labels\n",
        "ground_truth_labels = get_ground_truth_labels(museums)\n",
        "\n",
        "# Ensure labels are correctly formatted\n",
        "ground_truth_labels = [int(label) if isinstance(label, (int, str)) and str(label).isdigit() else 5 for label in ground_truth_labels]\n",
        "\n",
        "# Store results\n",
        "results = []\n",
        "\n",
        "# Process each prompt template\n",
        "for prompt_name, file_path in prompt_files.items():\n",
        "    predictions = load_predictions(file_path)\n",
        "    if predictions is None:\n",
        "        continue  # Skip missing files\n",
        "\n",
        "    # Process predictions: convert to integers and handle invalid cases\n",
        "    processed_predictions = [\n",
        "        int(pred) if isinstance(pred, (int, str)) and str(pred).isdigit() else 5\n",
        "        for pred in predictions\n",
        "    ]\n",
        "\n",
        "    # Ensure predictions and ground truth are the same length\n",
        "    min_len = min(len(processed_predictions), len(ground_truth_labels))\n",
        "    processed_predictions = processed_predictions[:min_len]\n",
        "    gt_labels = ground_truth_labels[:min_len]\n",
        "\n",
        "    # Compute metrics\n",
        "    accuracy = accuracy_score(gt_labels, processed_predictions)\n",
        "    precision = precision_score(gt_labels, processed_predictions, average=\"macro\", zero_division=0)\n",
        "    recall = recall_score(gt_labels, processed_predictions, average=\"macro\", zero_division=0)\n",
        "    f1 = f1_score(gt_labels, processed_predictions, average=\"macro\", zero_division=0)\n",
        "\n",
        "    # Store results\n",
        "    results.append([prompt_name, accuracy, precision, recall, f1])\n",
        "\n",
        "# Convert results to DataFrame for visualization\n",
        "results_df = pd.DataFrame(results, columns=[\"Prompt Template\", \"Accuracy\", \"Macro Precision\", \"Macro Recall\", \"Macro F1\"])\n",
        "\n",
        "# Display results\n",
        "print(results_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A2-4jBE39nJD"
      },
      "source": [
        "Prompt 1 has 0 values for all metrics, probably because all its examples were invalid and assigned to the sixth class. Prompt 2 has the highest performance everywhere. The 76% Accuracy indicates a good overall classification and the High Macro Precision shows that this prompt is confident enough. The 75% Macro Recall shows that Prompt 2 captures a big proportion of the true positives, while the 74% relatively high F1 score can confirm a strong balance between precision and recall. On the other hand, Prompt 3 was outperformed by Prompt 2, slightly in Accuracy and Macro Precision. Its Macro F1 of 58% indicates that it's less balanced than Prompt 2. These results indicate that probably Prompt 3 is less structured than Prompt 2.\n",
        "Now I will print the 3 first examples for each prompt, to mostly understand the nature of Prompt 1:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O1MkbdcR7ASa",
        "outputId": "979a7402-2836-4442-f478-4a09a2137fd5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------\n",
            "First 3 examples from Prompt 1:\n",
            "[\n",
            "  \"Based\",\n",
            "  \"Based\",\n",
            "  \"Based\"\n",
            "]\n",
            "--------------------------------------\n",
            "First 3 examples from Prompt 2:\n",
            "[\n",
            "  \"1\",\n",
            "  \"2\",\n",
            "  \"3\"\n",
            "]\n",
            "--------------------------------------\n",
            "First 3 examples from Prompt 3:\n",
            "[\n",
            "  \"1\",\n",
            "  \"2\",\n",
            "  \"1\"\n",
            "]\n"
          ]
        }
      ],
      "source": [
        "# Print first 3 examples from each prompt file\n",
        "for prompt_name, file_path in prompt_files.items():\n",
        "    predictions = load_predictions(file_path)\n",
        "\n",
        "    print('--------------------------------------')\n",
        "    print(f'First 3 examples from {prompt_name}:')\n",
        "    print(json.dumps(predictions[:3], indent=2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wfwoVFPy_ULT"
      },
      "source": [
        "It is clear now why Prompt 1 has values of 0 on all metrics. Its labels are not integers but strings, compared to the other prompts. In my dataset before I converted all lables to integers, and for this all the examples from Prompt 1 were considered invalid and assigned to the sixth class."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "odNkTD65_sb1"
      },
      "source": [
        "##Fine-tune a transformer\n",
        "Fine-tune a \u2018bert-base-uncased\u2019 transformer model on the model using the training set. You should use an\n",
        "AutoModelForSequenceClassification and the HuggingFace trainer. Use 8 epochs, a learning_rate of 5e-5 and a batch\n",
        "size of 8.\n",
        "Evaluate on the validation set. Report the per-class precision, recall and F1 score as well as the accuracy, macro\n",
        "precision, macro recall and macro F1 score. Don\u2019t be surprised with poor performance.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AJqR9VLSKOQO",
        "outputId": "25b24edf-8570-40cf-d113-87adc48e9d0f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU Available: True\n",
            "GPU Name: Tesla T4\n"
          ]
        }
      ],
      "source": [
        "# To check if GPU is being used\n",
        "import torch\n",
        "print(\"GPU Available:\", torch.cuda.is_available())\n",
        "print(\"GPU Name:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"No GPU\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "aec7e6d2ae774ea9a964d07296f319fd",
            "8ae5a37c5d2a44e7ad50ebb9dd22970e",
            "1af5af4803224ffaa47c8f3f3d8d67f5",
            "86cc3d8d91e4416d8487215d0f063313",
            "33e50df941a0474c8bd97a77764277d9",
            "6a514c2e573d47d7bf9f0f1b192525bf",
            "f0e0a8f8a32c40919920d3dd7c907c9d",
            "8f1f4e0458514eb8b130d653169a4689",
            "f7a1b9d3974343288c68490f8b18d050",
            "64b3f826693047138617451725bd3c75",
            "2532a876474343338f4a123e2e5cff6f",
            "a64d4eca778148b3ac99cc1d57c8d9a8",
            "937b0f96aa5042a79ea3ef34aff35104",
            "11c5410d44b848d9989aefee6f58c0e1",
            "d3d71c110d944d13bbb340a04d9ecba1",
            "8a653d49477846eea53bf62f90491fc7",
            "8bce8791784a410bb6e6e4500a8c2045",
            "d3ad48dba6014f53a17dc2916ef1db89",
            "ace98a9eff3149c08e13137139d24bb1",
            "adc10c6b457e42d996eb2dd3e9c8a263",
            "f9fdc8b5c51d48539c2bde036b69de7b",
            "9a0cb81d4296477481921a0bdf1473fd"
          ]
        },
        "id": "03lbbSMJPYBm",
        "outputId": "a8ab4cd9-1d33-415f-e093-c027ab958bfb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (2.14.4)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.52.4)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.15)\n",
            "Requirement already satisfied: fsspec>=2021.11.1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2021.11.1->datasets) (2025.3.2)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.15)\n",
            "Requirement already satisfied: huggingface-hub<1.0.0,>=0.14.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.32.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.15.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.4.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.20.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (4.14.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (1.1.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->datasets) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->datasets) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->datasets) (2025.4.26)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "<ipython-input-14-189898844d44>:104: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='152' max='152' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [152/152 03:27, Epoch 8/8]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>Macro Precision</th>\n",
              "      <th>Macro Recall</th>\n",
              "      <th>Macro F1</th>\n",
              "      <th>Per Class Metrics</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>No log</td>\n",
              "      <td>1.614726</td>\n",
              "      <td>0.340000</td>\n",
              "      <td>0.227636</td>\n",
              "      <td>0.363636</td>\n",
              "      <td>0.260606</td>\n",
              "      <td>{'class_0': {'precision': 0.32, 'recall': 1.0, 'f1': 0.48484848484848486, 'support': 8}, 'class_1': {'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'support': 13}, 'class_2': {'precision': 0.8181818181818182, 'recall': 0.8181818181818182, 'f1': 0.8181818181818182, 'support': 11}, 'class_3': {'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'support': 10}, 'class_4': {'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'support': 8}}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>No log</td>\n",
              "      <td>1.981259</td>\n",
              "      <td>0.220000</td>\n",
              "      <td>0.157143</td>\n",
              "      <td>0.200000</td>\n",
              "      <td>0.176000</td>\n",
              "      <td>{'class_0': {'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'support': 8}, 'class_1': {'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'support': 13}, 'class_2': {'precision': 0.7857142857142857, 'recall': 1.0, 'f1': 0.88, 'support': 11}, 'class_3': {'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'support': 10}, 'class_4': {'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'support': 8}}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>No log</td>\n",
              "      <td>2.435967</td>\n",
              "      <td>0.240000</td>\n",
              "      <td>0.208333</td>\n",
              "      <td>0.225000</td>\n",
              "      <td>0.216304</td>\n",
              "      <td>{'class_0': {'precision': 0.125, 'recall': 0.125, 'f1': 0.125, 'support': 8}, 'class_1': {'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'support': 13}, 'class_2': {'precision': 0.9166666666666666, 'recall': 1.0, 'f1': 0.9565217391304348, 'support': 11}, 'class_3': {'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'support': 10}, 'class_4': {'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'support': 8}}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>No log</td>\n",
              "      <td>3.043426</td>\n",
              "      <td>0.200000</td>\n",
              "      <td>0.181818</td>\n",
              "      <td>0.181818</td>\n",
              "      <td>0.181818</td>\n",
              "      <td>{'class_0': {'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'support': 8}, 'class_1': {'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'support': 13}, 'class_2': {'precision': 0.9090909090909091, 'recall': 0.9090909090909091, 'f1': 0.9090909090909091, 'support': 11}, 'class_3': {'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'support': 10}, 'class_4': {'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'support': 8}}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>No log</td>\n",
              "      <td>3.940183</td>\n",
              "      <td>0.200000</td>\n",
              "      <td>0.181818</td>\n",
              "      <td>0.181818</td>\n",
              "      <td>0.181818</td>\n",
              "      <td>{'class_0': {'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'support': 8}, 'class_1': {'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'support': 13}, 'class_2': {'precision': 0.9090909090909091, 'recall': 0.9090909090909091, 'f1': 0.9090909090909091, 'support': 11}, 'class_3': {'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'support': 10}, 'class_4': {'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'support': 8}}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>No log</td>\n",
              "      <td>4.204368</td>\n",
              "      <td>0.200000</td>\n",
              "      <td>0.166667</td>\n",
              "      <td>0.181818</td>\n",
              "      <td>0.173913</td>\n",
              "      <td>{'class_0': {'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'support': 8}, 'class_1': {'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'support': 13}, 'class_2': {'precision': 0.8333333333333334, 'recall': 0.9090909090909091, 'f1': 0.8695652173913043, 'support': 11}, 'class_3': {'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'support': 10}, 'class_4': {'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'support': 8}}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>No log</td>\n",
              "      <td>4.436380</td>\n",
              "      <td>0.200000</td>\n",
              "      <td>0.166667</td>\n",
              "      <td>0.181818</td>\n",
              "      <td>0.173913</td>\n",
              "      <td>{'class_0': {'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'support': 8}, 'class_1': {'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'support': 13}, 'class_2': {'precision': 0.8333333333333334, 'recall': 0.9090909090909091, 'f1': 0.8695652173913043, 'support': 11}, 'class_3': {'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'support': 10}, 'class_4': {'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'support': 8}}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>No log</td>\n",
              "      <td>4.498043</td>\n",
              "      <td>0.200000</td>\n",
              "      <td>0.166667</td>\n",
              "      <td>0.181818</td>\n",
              "      <td>0.173913</td>\n",
              "      <td>{'class_0': {'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'support': 8}, 'class_1': {'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'support': 13}, 'class_2': {'precision': 0.8333333333333334, 'recall': 0.9090909090909091, 'f1': 0.8695652173913043, 'support': 11}, 'class_3': {'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'support': 10}, 'class_4': {'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'support': 8}}</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='7' max='7' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [7/7 00:01]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======= Overall Metrics =======\n",
            "Accuracy: 0.340\n",
            "Macro Precision: 0.228\n",
            "Macro Recall: 0.364\n",
            "Macro F1-score: 0.261\n",
            "\n",
            "======= Metrics for each Class =======\n",
            "No per-class metrics available.\n"
          ]
        }
      ],
      "source": [
        "!pip install datasets transformers scikit-learn\n",
        "\n",
        "import os\n",
        "import torch\n",
        "from datasets import Dataset\n",
        "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer, AutoTokenizer, DataCollatorWithPadding\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
        "from transformers import set_seed\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "SEED = 111\n",
        "\n",
        "# Ensure all randomness sources are controlled\n",
        "os.environ[\"PYTHONHASHSEED\"] = str(SEED)\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed(SEED)\n",
        "torch.cuda.manual_seed_all(SEED)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "\n",
        "# Hugging Face transformers seed setup\n",
        "set_seed(SEED)\n",
        "\n",
        "# Load the dataset\n",
        "# Assuming 'museums' is already loaded as a dictionary containing 'train' and 'val' lists of dictionaries\n",
        "train_texts = [record[\"content\"] for record in museums[\"train\"]]\n",
        "train_labels = [record[\"label\"] for record in museums[\"train\"]]\n",
        "val_texts = [record[\"content\"] for record in museums[\"val\"]]\n",
        "val_labels = [record[\"label\"] for record in museums[\"val\"]]\n",
        "\n",
        "# Convert dataset to Hugging Face Dataset format\n",
        "train_dataset = Dataset.from_dict({\"text\": train_texts, \"label\": train_labels})\n",
        "val_dataset = Dataset.from_dict({\"text\": val_texts, \"label\": val_labels})\n",
        "\n",
        "# Load tokenizer\n",
        "MODEL_NAME = \"bert-base-uncased\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "\n",
        "# Tokenization function\n",
        "def tokenize_function(example):\n",
        "    return tokenizer(example[\"text\"], padding=\"max_length\", truncation=True, max_length=512)\n",
        "\n",
        "# Tokenize datasets\n",
        "train_dataset = train_dataset.map(tokenize_function, batched=True, remove_columns=[\"text\"])\n",
        "val_dataset = val_dataset.map(tokenize_function, batched=True, remove_columns=[\"text\"])\n",
        "\n",
        "# Load Model\n",
        "num_labels = len(set(train_labels))  # Number of unique labels\n",
        "model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=num_labels)\n",
        "\n",
        "# Define Evaluation Metrics\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    predictions = logits.argmax(axis=1)\n",
        "\n",
        "    accuracy = accuracy_score(labels, predictions)\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(labels, predictions, average=\"macro\", zero_division=0)\n",
        "\n",
        "    # Modified to unpack 4 values (precision, recall, f1, support)\n",
        "    # Ensure labels cover all classes expected by precision_recall_fscore_support\n",
        "    unique_labels = sorted(list(set(labels) | set(predictions)))\n",
        "    if not unique_labels: # Handle case where there are no labels\n",
        "        per_class_metrics = {}\n",
        "    else:\n",
        "        per_class_precision, per_class_recall, per_class_f1, per_class_support = precision_recall_fscore_support(labels, predictions, average=None, zero_division=0, labels=unique_labels)\n",
        "\n",
        "        per_class_metrics = {\n",
        "            f\"class_{unique_labels[i]}\": {\"precision\": p, \"recall\": r, \"f1\": f, \"support\": s}\n",
        "            for i, (p, r, f, s) in enumerate(zip(\n",
        "                per_class_precision, per_class_recall, per_class_f1, per_class_support\n",
        "            ))\n",
        "        }\n",
        "\n",
        "\n",
        "    return {\n",
        "        \"accuracy\": accuracy,\n",
        "        \"macro_precision\": precision,\n",
        "        \"macro_recall\": recall,\n",
        "        \"macro_f1\": f1,\n",
        "        \"per_class_metrics\": per_class_metrics\n",
        "    }\n",
        "\n",
        "# Define Training Arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"model\",  # Creates a unique folder to prevent overwriting older models\n",
        "    eval_strategy=\"epoch\",  # Corrected parameter name\n",
        "    save_strategy=\"epoch\",  # Save at the end of each epoch\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    num_train_epochs=8,  # As required in the coursework\n",
        "    learning_rate=5e-5,  # As required\n",
        "    report_to=\"none\",  # Suppress logging\n",
        "    # Add load_best_model_at_end and metric_for_best_model for the next step\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"macro_f1\", # Or \"eval_macro_f1\" depending on transformers version\n",
        "    greater_is_better=True,\n",
        ")\n",
        "\n",
        "# Prepare Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    data_collator=DataCollatorWithPadding(tokenizer=tokenizer),\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics\n",
        ")\n",
        "\n",
        "# Train Model\n",
        "trainer.train()\n",
        "\n",
        "# Evaluate Model\n",
        "eval_results = trainer.evaluate()\n",
        "\n",
        "# Extract and Print Results\n",
        "overall_metrics = {\n",
        "    \"Accuracy\": eval_results[\"eval_accuracy\"],\n",
        "    \"Macro Precision\": eval_results[\"eval_macro_precision\"],\n",
        "    \"Macro Recall\": eval_results[\"eval_macro_recall\"],\n",
        "    \"Macro F1-score\": eval_results[\"eval_macro_f1\"]\n",
        "}\n",
        "\n",
        "# Remove 'eval_' prefix for per-class metrics and convert to DataFrame\n",
        "per_class_data = {k.replace('eval_per_class_metrics_', ''): v for k, v in eval_results.items() if k.startswith('eval_per_class_metrics_')}\n",
        "per_class_metrics = pd.DataFrame.from_dict(per_class_data, orient=\"index\")\n",
        "\n",
        "\n",
        "# Print Overall Metrics\n",
        "print(\"\\n======= Overall Metrics =======\")\n",
        "for metric, value in overall_metrics.items():\n",
        "    print(f\"{metric}: {value:.3f}\")\n",
        "\n",
        "# Print Per-Class Metrics in Table Format\n",
        "print(\"\\n======= Metrics for each Class =======\")\n",
        "# Adjusting based on the structure after the fix\n",
        "if not per_class_metrics.empty:\n",
        "     print(per_class_metrics.to_string(float_format=\"%.3f\"))\n",
        "else:\n",
        "    print(\"No per-class metrics available.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gKiJImR2VcyN"
      },
      "source": [
        "The performance is very bad. The 20% accuracy indicates that the classifier predicts correctly 1 out of 5 times. Probably there is something wrong with the validation dataset.\n",
        "Also, only class 2 presented good results, while all the other classes had values of 0 on their metrics."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R5n2tbFWV6pK"
      },
      "source": [
        "##A problem with the validation set\n",
        "There is an issue with the validation set which causes poor performance. Provide the confusion matrix. Describe the\n",
        "problem, how you identified it and how you fixed it.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        },
        "id": "NVaQEyxHWAQ0",
        "outputId": "1302fc70-4ebb-46b8-b7a1-ac3eda9ab2a8"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x600 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "# Get model predictions for validation dataset\n",
        "predictions = trainer.predict(val_dataset)\n",
        "predicted_labels = np.argmax(predictions.predictions, axis=1)\n",
        "true_labels = [record[\"label\"] for record in museums[\"val\"]]\n",
        "\n",
        "# Compute confusion matrix\n",
        "cm = confusion_matrix(true_labels, predicted_labels)\n",
        "\n",
        "# Plot confusion matrix\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=sorted(set(true_labels)), yticklabels=sorted(set(true_labels)))\n",
        "plt.xlabel(\"Predicted Label\")\n",
        "plt.ylabel(\"True Label\")\n",
        "plt.title(\"Confusion Matrix - Validation Set\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wBH8yheeRIaj"
      },
      "source": [
        "There is a clear problem of label mismatching on the validation dataset. Labels are shifted or were assigned incorrectly. The matches should be on the main diagonal, but the opposite is happening. For example, the true label of class 0 is almost entirely predicted as class 4. For class 2, which is exactly in the middle of the confusion matrix, nothing changed in the prediction because it keeps the same position even if the diagonal changes direction. That's why class 2 is the only one that presented high accuracy before.\n",
        "For this, I am going to remap the labels to their correct ones, since it seems that they were classified to the opposite direction:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 599
        },
        "id": "dquC-7KtUDq_",
        "outputId": "bab708e5-c63d-473b-d681-09a4ad4e75cb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Corrected Validation Set Label Distribution: Counter({3: 13, 2: 11, 1: 10, 0: 8, 4: 8})\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x600 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Corrected Validation Set Label Distribution: Counter({3: 13, 2: 11, 1: 10, 0: 8, 4: 8})\n"
          ]
        }
      ],
      "source": [
        "label_mapping = {0: 4, 1: 3, 2: 2, 3: 1, 4: 0}\n",
        "\n",
        "# Apply label remapping (keeping the format as a list)\n",
        "true_labels_fixed = [label_mapping[label] for label in true_labels]\n",
        "\n",
        "# Recompute the confusion matrix with corrected labels\n",
        "cm_fixed = confusion_matrix(true_labels_fixed, predicted_labels)\n",
        "\n",
        "# Print to verify the label distribution\n",
        "from collections import Counter\n",
        "print(\"Corrected Validation Set Label Distribution:\", Counter(true_labels_fixed))\n",
        "\n",
        "# Plot the corrected confusion matrix\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm_fixed, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
        "            xticklabels=sorted(set(true_labels_fixed)),\n",
        "            yticklabels=sorted(set(true_labels_fixed)))\n",
        "plt.xlabel(\"Predicted Label\")\n",
        "plt.ylabel(\"True Label\")\n",
        "plt.title(\"Corrected Confusion Matrix - Validation Set\")\n",
        "plt.show()\n",
        "\n",
        "# Print to verify the label distribution\n",
        "from collections import Counter\n",
        "print(\"Corrected Validation Set Label Distribution:\", Counter(true_labels_fixed))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c-okoEBkUsSt"
      },
      "source": [
        "Now the validation test has the correct structure and it will perform better."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FmGqq0u_U2vD"
      },
      "source": [
        "##Hyperparameter tuning\n",
        "Train and evaluate several fine-tuned transformer models using the corrected training and validation sets. Try the\n",
        "four base models listed below. Use 8 epochs, a learning_rate of 5e-5 and a batch size of 8. Base models to try: 'bert-base-uncased','roberta-base','distilbert-base-uncased', and\n",
        "'microsoft/BiomedNLP-BiomedBERT-base-uncased-abstract'\n",
        "This time, we want the best model found during the training process for each base model and to save it for the final\n",
        "analysis. For example, if the model after 3 epochs is the best performing on the validation set (by macro-F1), we want\n",
        "to keep that. You should investigate the load_best_model_at_end parameter for the Trainer (which does require\n",
        "other parameters).\n",
        "Evaluate each fine-tuned model on the validation set. Report the per-class precision, recall and F1 score as well as the\n",
        "accuracy, macro precision, macro recall and macro F1 score. Comment on the performance of each model.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "1915f6a23f4448dcbe9e44c1d24e918c",
            "039846ae62fa4942a71cda8441abafa8",
            "a3e0caf18d46404a88c80a5d6993ff01",
            "1a09403c096346fd90e61e4d576f53c2",
            "5728d47656264025b137bef5e886bd85",
            "363bd8df11a64d4f9df32a392026cb1c",
            "cba508228a744e3c938dab2abeab364c",
            "48366b77e89f4ebca8b341064262fa49",
            "075913c977944c90b2d57aacf68e4f51",
            "6b48da7e54594e35b2837cd427d92451",
            "6ff219d44c444e559ed1712498ba88fc",
            "f1b9dc7da84c4069b76e7fbefb003f68",
            "b7e63154233846b98fd28af0baff4033",
            "3e5bb44733364ba5aa3218ab3bf8abf2",
            "aab68e498af145dba012edcb13dc559f",
            "d45af8424a9c4f198337821547bca141",
            "2dac056fcea348579ec0b968d791968d",
            "5831ff6782bf4fdeadf1c2944230b2eb",
            "665be21296f144d2bb214036c75aa498",
            "0ad444ae61dd486496566da28bcff8e5",
            "90e93c1e60394d2ba89d341b9a407610",
            "7b91470c67904b98893332ee8a1707d6",
            "55cb5067fabc4f16aef47d4b5b69df31",
            "43c87487578147b5b0ffb6d3af792462",
            "a7961472400c45ffb8aad0aa4aa75475",
            "42c47ec208df4f0e950df361247ea04b",
            "f7d5c8903510497f8adaf1eaaf8cf16f",
            "207357a41988402db04cf5bc07272514",
            "99bd843c0d2f46908cc4e3775a58570e",
            "70daa6a746a14d978bb2040aedfb912e",
            "a7807544233945249e759d50bd5564e9",
            "a2e586a9386443ada9fd2f7f0238b2eb",
            "01e09a3c847447daa8554f2f9e55fbc2",
            "bd69eb985895465595e4b93c2f620049",
            "a2c41e235b9f4e458afa75a65c3de5ba",
            "f6de137c2149469b8ccb65322f22cb38",
            "19b9df1c401741d2afcc92aa2334bd93",
            "79943fb3d1d94feda0447edbe197f2fc",
            "082b8a9df1d040fcb55f3a956f613a72",
            "3484ddeb278045c0b73fbf2f62bcd3a4",
            "45ddbb3f1de540a4a14c93a3aaca7bd2",
            "5d74bcb0bddc4b608660dd00a7652fd8",
            "11f4eb311c804cb693d90a558f27a219",
            "fd0a4915c4f64e6b9ea9662ac64ab610",
            "267ee472315d4283b99e17ea426dc619",
            "d05db61b36dc4a22bdfa0a3359f145b6",
            "27833fecc58443c7a82f5236760a904f",
            "0b587d6cd0a04f72902db873316150d1",
            "13ce8e5fa36b421f830fc4ecbb715f1d",
            "5722ba5bc26d4a91826a6f66523b2a75",
            "d3cecd184fa848c0b6669da0bffe53d3",
            "a15c3702102548a1a2b660558f81645b",
            "b64e1604a7c246a59e109b49d0790db1",
            "903e1c22b028456fa5cd3c618104e1e3",
            "4c18211cee444ffa95739698f2d14a97",
            "c6215e64bd45437581e97e68aa78a538",
            "8bf624b259ae41ce92d8293d5aba9552",
            "fd0edd93880e49d5a30effe0afab81d0",
            "f592d0a595354b41ba90d198ba12ce5d",
            "fc0a1186c6f9487d97e2f7b14d4acd6e",
            "0a4dac4401074af7900fa30687f05874",
            "68ca9bc9c6d54405a24fa63b9fa6ca99",
            "adb8cd5554d94208832ceb327163fae0",
            "d119ea05a2ca48d5a3b91ba8e824c49d",
            "8ad82ad206374f3085cfa38e73185146",
            "8768f322ff6e4ea287132f336adb89b5",
            "b88c336ccb624ba985c3eae495278356",
            "eaf7a079eb9c48669178e7c901b1fc33",
            "79d4a694b56440eeba4c1ea738e1c226",
            "252fb372562346bb8bb31e26e51e4359",
            "44de33d5c092434c87c66955d70b58cc",
            "ffbe0f3edf7e47bf927d021052ae6ffe",
            "b64bfa8031764d99a78ff82f1b329c0d",
            "e8e00e76019a4ce7990a728a81a6c2a3",
            "bbb8b8d0b89648a09d5e976ace5ac459",
            "fbc1f0775302413e80ba2b2dd7c853da",
            "7c365da029f34db79b177fb81b06a66b",
            "c33f2c94746548e6934e2b8515969d9d",
            "b4dc8aa176b049388341a078c2e6dfe5",
            "16f9287f5201476195546e9a08052671",
            "2fd72091988f4065be42993a4b3be629",
            "77454ae2909a4e52b628611a36d5a4e5",
            "c934d4a9fea246ec8e82853e2e7eddfc",
            "1b0e44c79d344d89a0b876f16ea4dfab",
            "8917b3b6ee0f48b3886fdb7d3a293107",
            "26db48e383dc4d6cbf56ecac8f4be717",
            "e13df075a8c94593a944c50acee09a82",
            "27a2a174b4d942589cdd99693c68c33d",
            "ddb146e11bad4ec6a4ad3a54b53316e7",
            "7d3b6924bebd427688cdb25f47d27f1b",
            "9f4c891c3d7c4de9b20073ab5557803a",
            "7c876c4d05ac4f2dbd2a3755466a01b7",
            "91455bac8c0b4fc7a72d9188690c7aab",
            "6c0cbebfd8574b4f8fbb4c0df26f14ff",
            "9f2166bd2a9748208d056b241dd1f3b6",
            "1fea8676f86c4b518c5f1a557fbadfad",
            "6d9fe2155be846b996f571de697def4f",
            "618656394a064153b07d0dbaf6b833be",
            "fb24c270e71e461981901eed6d79843f",
            "cbcb35ecd1c4408d8a7749d2f6cae09c",
            "3d136dc10f3a49108a5c7a702d928b55",
            "478d914cb5464a958bd212974256a815",
            "14cbc5d952cd4bdabbcb39487c4c682d",
            "86c6077b64b1498cb80d48eeb8dbefd5",
            "37cd7de07d6141038bf5c82faad9442b",
            "6878f1607d6844a0b31321a927ce1484",
            "8767de9b2f0746c4a10b8543b3c64fbf",
            "c59ec90ac936438ebf78c63797f597f5",
            "32e6483e5e9f460cb87efa31a0ddaad6",
            "3bfbbd34657245a2a5c588107490299f",
            "02b7a7c87b7e48b5b719cecfd23141cf",
            "20d15f606a8b46f3808a886ca297b78c",
            "8cfa32b848544495827c41ad83f0ae03",
            "4b2eda28f7f340f9b9146b91ecc657ce",
            "af88d6e61ced4d32986aa516bf08ade0",
            "5377a5df98c0466d922c8b18b09c184f",
            "9d5547cf60074bcfa8ee15d9855e5f6b",
            "8547de849c3448ab9d06b8cd4e66bcd1",
            "d3169c6ee9cd46fb99f9201a6d13df8e",
            "8b5212d722d2479c9d52ab35fb142c2d",
            "0a5b1725d65846e284dce9ab4e3957ff",
            "acaaed7be9b44e1780984c81cafd74bb",
            "e6ed3ed61d594652982b8b432a095ebf",
            "c4bf06a7ea0848e5b149b2b4e383fd31",
            "e9253e6aeac84dc0896af25d29e5d3d4",
            "0730d8ffd1fb46428b212b0baeccc8b2",
            "c28926e18f9246d3866aa1b97b28940d",
            "2fd3ebf98b434407a585b28de8781f22",
            "34015e182def4064b495f6f44ebd396d",
            "3ae407a11d234e96882394ac3ed3fb84",
            "2c0684d5ac1f4a51869bf74e86f9236f",
            "193fe33bb150463290a4255fdc6539e8",
            "4df91d75f64945afab045a84adab7d12",
            "16bf88c610d245fda52e10dd12148e7a",
            "ffffc3874cd84da484e3ea1e0496fef1",
            "8b5d3d9e33a1411aab2e0f1402359f0d",
            "5bee0ab516674683ae48c9c2da1b5a11",
            "7378840bc18343748de9617acfb6a03b",
            "6a9d9d6874f740c6b1ad5d359ac785af",
            "dbc8d73152e04167a58b563429ed1aa6",
            "71ae09d26a5646e1bb914baf25492359",
            "90804b3aa4e3470fa9c4471db15ff708",
            "33b0148f24f84ae691c27440cdc4ef06",
            "8d0b912cbb03495588e3731cdf6a6c31",
            "8b6f919c3b7c4fd4bf3d2f317c6ade07",
            "7155549bcc92449f9435cc133403ea66",
            "4cc21c19a4964ae3b755a4e1ca9ab537",
            "445bd3e9336f4375b51cf45e89ac7c88",
            "39e5153ce0d044dfabdf345c936d9d50",
            "bd19e77e7384430e97f11d25fd89656f",
            "2d991a88048d41578d7b78da50e96672",
            "777459e389534f89b6ab557829f94e71",
            "c7c8ef6de4a84cb9af5034d1207b106c",
            "b805ffbe20ab4bdea941fcd2afae63e1",
            "eb9258ee854e4705b837d597b814ccaf",
            "5a89d173d88d451eb7c5704312661ba3",
            "55f7e3ba9a8e49ac82dc58cc0f7d25d1",
            "850bc83c3cd1450bb7b331a78ef4d7c0",
            "cb47c58a05a54b8d992832a324e78088",
            "2f0ff2743ce248268ee55ad9c94f4b15",
            "8265958f194546d9a30479c88fc24287",
            "b321fbbab21841ccaaf75dd27e863629",
            "7ec2acf5805d46c7b1c4df64342e4355",
            "9f44378afd524b109a03372d769cbeea",
            "3873ceb1b8dd46fc924fbf15350f4427",
            "94c7bed25fa343309134806c2da6faad",
            "5786b58d42e24d02acfbc618423dcf08",
            "7b8209c50daa4e0bb1646c64f6f2eda9",
            "9c2c8b6d7c7a415882e3e645fbc6d0ce",
            "ab3c514b9e2e48558142a94d07312e14",
            "29bb63a9f6264ab5b9fe2efe2fdfe574",
            "445cefff859b4772a122bc4f8e48c9c5",
            "be85c9889bb945ba8c18f4227fa8a93a",
            "822281d57ddf4b0abba0083a025fb811",
            "14c7f3b659db4ce8b15079627ca23338",
            "548f6e4f7cf740d2a711f89753814c56",
            "b91ff9be1c1541f8b97a93decd4394f1",
            "c44d8d14cc3d44888e39fe280b9068c5",
            "c7e53a1c7e694bc9963467c8af7686de",
            "10990e5b26ca4ef2b38ac3e53c27565d",
            "232ab27f805a4c6b9ff6dd695993e0a0",
            "4a510d5909ae40208b7a6305bb2a47ce",
            "35af07eebcde498fa73e769f264d26ee",
            "076207c25ca249c3b321002f8c84a6da",
            "7eca52df3d4f41268564e3f2fe285843",
            "9cfa757c2abf4861bc3780193fcaa598",
            "a4bfaaefd5ff4a6e944aec7a9831c605",
            "9ac68282d15a4d2987af17038aa4d07a",
            "c4d71f1d2be54c2f89c876680edf1da5",
            "db4ba0277e63469a8bdc99b834e16cfe",
            "772e2d210a6742f1b7e27790acfb2756",
            "2c35c34e16d64e5791755a8d221e3022",
            "395ae0dcb93a41faa502314cee8e03d6",
            "c3f8879f8bba497a920a11c6a65e2325",
            "4a106c6ed93644a4a52e26bb4feb570c",
            "a9523897364349aaac265b94cb5d87dc",
            "7acec1444fa4458599ad94502888c974",
            "80641a7c57c84dc0a2842ff972443fc5",
            "842b7a68d8fb482fbe454a3a431945ba",
            "7a37368d88ae4dcbb6033365c1493bbb",
            "20ca616867da49a98417318208bd7098",
            "f4cdc6a9558143e18c933e0d11fe8dd4",
            "714b708bb2594f698fc6f58ded311b94",
            "1500edb266444ca783b0a4579616b4cb",
            "306cbd0d7eb144a1adbc5a7b4792fdd6",
            "c0a824620193420d9145ef8dd472ef14",
            "50e88d3345eb456e999fe8810d33c52c",
            "2d115341792140a0872ddd8359f48b42",
            "8b96a8eaf1f0422987493e570bcd087c",
            "99e23fdb6e624585ae75a601b6d42d1f",
            "5cdb61b7ac1c434b84d5e20bbfe003bd",
            "7bf0da1706554057b18502475117257d",
            "0804c3000d05460f942e27170ae127ef",
            "4f17ed01ea9844f0b949965eeb897622",
            "5a9e183136bd4373a5644d23ce2aa36c",
            "61cf1767a3fa449bb6c88b5cb308dfad",
            "c491127b00eb4dfb8d1913325d3c3589",
            "d13239298d7f46d4ab94e403d51e145a",
            "7d6d057bd5094d5a859b8c26807afb75",
            "21129e149e7147c7ad00a572caa84858",
            "94491f518c454fdcab6bab7a40d8d4c5",
            "ccbebd4c71f24002b45f9ae3fe4b52c9",
            "538a4cd99b2d4c349a2ecc0fac28f8ef",
            "88b2747308ee4304bfc8339159f77160",
            "0febe2f697f64b479dab3635b758b5ad",
            "d25b0dbf8eee4fc0bf532c246a34863b",
            "72b368e89c1c4f218fccb95739c7c76c",
            "1217850627df43a69bb9658993581e10",
            "de98c5a63be24bee8e3b07417e7cb792",
            "72d0a4f557f34a2e9dc8b3410cc9de85",
            "3e9ad6ea999640ccbc160e6f17041bf9",
            "17ce8e44d88c49f29d5ac354792d8a35",
            "5cffccebfe514a8fb11ece051321e961",
            "465a561403f748c1a9684fe9a356ba25",
            "a98b4da15eae46f1975d60f717885a03",
            "ee71292960e14d19b237fbbd46400de9",
            "61ce57ed6d1c43f4bf8d566e5de5b5f2",
            "4efe0eff4cb84b9996acb3791f2c39b9",
            "a19d9e8e6dc54a6095c7529665453e62",
            "bfb1ca595ea545b9a5942672a7dab1ad",
            "80918ecbc0a44edc800dc8b6b1ed2b26",
            "b398734b99494e9085568f3ced10dd6d",
            "f40066188a0d404da1e2ce8b12712c57",
            "a02df315402244078671f360d6d48b63",
            "d0615bc03d324a198863c57591de44e6",
            "0a87a16053924718913bc9128c7d54a2",
            "02d539dc28494f8c91d30d00cb7d0565",
            "c1670bd99f8148c18958b8328e9c3158",
            "7752116c769f41a7a342c736d47c7216",
            "e2ef9100ef57417da4c78250366683c2",
            "7a145aab27a14dae947305b724a6893b",
            "218ec2dfc2d04e98bff8b97eb984c60e",
            "df55e7afee5f4eb7ac4c19ae47bb579f",
            "440edf981b81434182671cfbbb254d2b",
            "e14c226b85c7422189e92f0eb9fb1224",
            "405c468e754f406797539ba01310311a",
            "ec390cf2a9e14be7bd7a63d378906797",
            "1cae2df0fce44c2aaffc15c3330ef37d",
            "0e142163181344a1969bf1a59e951c40",
            "0cff5f31f7aa4ab1a24881e789ffa922",
            "3ac0f989d4964333b6eba8cda5c4fe33",
            "49d990dcce464af0a6adfe3e48f176cc",
            "95913cba991a43998450d50393bc442c",
            "8be2fbfd04384bbaaf8e9867fce95163"
          ]
        },
        "id": "uIc5OjoY19HD",
        "outputId": "c9288dd9-3721-4905-8b0f-1c5fae0b3a9f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training bert-base-uncased...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-18-17b176282799>:86: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='152' max='152' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [152/152 04:34, Epoch 8/8]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>Macro Precision</th>\n",
              "      <th>Macro Recall</th>\n",
              "      <th>Macro F1</th>\n",
              "      <th>Per Class Metrics</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>No log</td>\n",
              "      <td>1.117549</td>\n",
              "      <td>0.600000</td>\n",
              "      <td>0.609455</td>\n",
              "      <td>0.609790</td>\n",
              "      <td>0.526082</td>\n",
              "      <td>{'class_0': {'precision': 0.32, 'recall': 1.0, 'f1': 0.48484848484848486, 'support': 8}, 'class_1': {'precision': 0.9090909090909091, 'recall': 1.0, 'f1': 0.9523809523809523, 'support': 10}, 'class_2': {'precision': 0.8181818181818182, 'recall': 0.8181818181818182, 'f1': 0.8181818181818182, 'support': 11}, 'class_3': {'precision': 1.0, 'recall': 0.23076923076923078, 'f1': 0.375, 'support': 13}, 'class_4': {'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'support': 8}}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.700698</td>\n",
              "      <td>0.900000</td>\n",
              "      <td>0.926374</td>\n",
              "      <td>0.894231</td>\n",
              "      <td>0.903326</td>\n",
              "      <td>{'class_0': {'precision': 1.0, 'recall': 0.875, 'f1': 0.9333333333333333, 'support': 8}, 'class_1': {'precision': 1.0, 'recall': 1.0, 'f1': 1.0, 'support': 10}, 'class_2': {'precision': 0.7857142857142857, 'recall': 1.0, 'f1': 0.88, 'support': 11}, 'class_3': {'precision': 0.8461538461538461, 'recall': 0.8461538461538461, 'f1': 0.8461538461538461, 'support': 13}, 'class_4': {'precision': 1.0, 'recall': 0.75, 'f1': 0.8571428571428571, 'support': 8}}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.384544</td>\n",
              "      <td>0.920000</td>\n",
              "      <td>0.929762</td>\n",
              "      <td>0.909615</td>\n",
              "      <td>0.915511</td>\n",
              "      <td>{'class_0': {'precision': 0.875, 'recall': 0.875, 'f1': 0.875, 'support': 8}, 'class_1': {'precision': 1.0, 'recall': 1.0, 'f1': 1.0, 'support': 10}, 'class_2': {'precision': 0.9166666666666666, 'recall': 1.0, 'f1': 0.9565217391304348, 'support': 11}, 'class_3': {'precision': 0.8571428571428571, 'recall': 0.9230769230769231, 'f1': 0.8888888888888888, 'support': 13}, 'class_4': {'precision': 1.0, 'recall': 0.75, 'f1': 0.8571428571428571, 'support': 8}}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.231681</td>\n",
              "      <td>0.920000</td>\n",
              "      <td>0.941818</td>\n",
              "      <td>0.916434</td>\n",
              "      <td>0.926580</td>\n",
              "      <td>{'class_0': {'precision': 1.0, 'recall': 0.875, 'f1': 0.9333333333333333, 'support': 8}, 'class_1': {'precision': 1.0, 'recall': 1.0, 'f1': 1.0, 'support': 10}, 'class_2': {'precision': 0.9090909090909091, 'recall': 0.9090909090909091, 'f1': 0.9090909090909091, 'support': 11}, 'class_3': {'precision': 0.8, 'recall': 0.9230769230769231, 'f1': 0.8571428571428571, 'support': 13}, 'class_4': {'precision': 1.0, 'recall': 0.875, 'f1': 0.9333333333333333, 'support': 8}}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.155288</td>\n",
              "      <td>0.940000</td>\n",
              "      <td>0.953247</td>\n",
              "      <td>0.941434</td>\n",
              "      <td>0.946263</td>\n",
              "      <td>{'class_0': {'precision': 1.0, 'recall': 0.875, 'f1': 0.9333333333333333, 'support': 8}, 'class_1': {'precision': 1.0, 'recall': 1.0, 'f1': 1.0, 'support': 10}, 'class_2': {'precision': 0.9090909090909091, 'recall': 0.9090909090909091, 'f1': 0.9090909090909091, 'support': 11}, 'class_3': {'precision': 0.8571428571428571, 'recall': 0.9230769230769231, 'f1': 0.8888888888888888, 'support': 13}, 'class_4': {'precision': 1.0, 'recall': 1.0, 'f1': 1.0, 'support': 8}}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.164891</td>\n",
              "      <td>0.920000</td>\n",
              "      <td>0.938095</td>\n",
              "      <td>0.916434</td>\n",
              "      <td>0.925024</td>\n",
              "      <td>{'class_0': {'precision': 1.0, 'recall': 0.875, 'f1': 0.9333333333333333, 'support': 8}, 'class_1': {'precision': 1.0, 'recall': 1.0, 'f1': 1.0, 'support': 10}, 'class_2': {'precision': 0.8333333333333334, 'recall': 0.9090909090909091, 'f1': 0.8695652173913043, 'support': 11}, 'class_3': {'precision': 0.8571428571428571, 'recall': 0.9230769230769231, 'f1': 0.8888888888888888, 'support': 13}, 'class_4': {'precision': 1.0, 'recall': 0.875, 'f1': 0.9333333333333333, 'support': 8}}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.172592</td>\n",
              "      <td>0.920000</td>\n",
              "      <td>0.938095</td>\n",
              "      <td>0.916434</td>\n",
              "      <td>0.925024</td>\n",
              "      <td>{'class_0': {'precision': 1.0, 'recall': 0.875, 'f1': 0.9333333333333333, 'support': 8}, 'class_1': {'precision': 1.0, 'recall': 1.0, 'f1': 1.0, 'support': 10}, 'class_2': {'precision': 0.8333333333333334, 'recall': 0.9090909090909091, 'f1': 0.8695652173913043, 'support': 11}, 'class_3': {'precision': 0.8571428571428571, 'recall': 0.9230769230769231, 'f1': 0.8888888888888888, 'support': 13}, 'class_4': {'precision': 1.0, 'recall': 0.875, 'f1': 0.9333333333333333, 'support': 8}}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.172477</td>\n",
              "      <td>0.920000</td>\n",
              "      <td>0.938095</td>\n",
              "      <td>0.916434</td>\n",
              "      <td>0.925024</td>\n",
              "      <td>{'class_0': {'precision': 1.0, 'recall': 0.875, 'f1': 0.9333333333333333, 'support': 8}, 'class_1': {'precision': 1.0, 'recall': 1.0, 'f1': 1.0, 'support': 10}, 'class_2': {'precision': 0.8333333333333334, 'recall': 0.9090909090909091, 'f1': 0.8695652173913043, 'support': 11}, 'class_3': {'precision': 0.8571428571428571, 'recall': 0.9230769230769231, 'f1': 0.8888888888888888, 'support': 13}, 'class_4': {'precision': 1.0, 'recall': 0.875, 'f1': 0.9333333333333333, 'support': 8}}</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='7' max='7' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [7/7 00:01]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluation results for bert-base-uncased:\n",
            "{'eval_loss': 0.15528768301010132, 'eval_accuracy': 0.94, 'eval_macro_precision': 0.9532467532467532, 'eval_macro_recall': 0.9414335664335665, 'eval_macro_f1': 0.9462626262626262, 'eval_per_class_metrics': {'class_0': {'precision': 1.0, 'recall': 0.875, 'f1': 0.9333333333333333, 'support': 8}, 'class_1': {'precision': 1.0, 'recall': 1.0, 'f1': 1.0, 'support': 10}, 'class_2': {'precision': 0.9090909090909091, 'recall': 0.9090909090909091, 'f1': 0.9090909090909091, 'support': 11}, 'class_3': {'precision': 0.8571428571428571, 'recall': 0.9230769230769231, 'f1': 0.8888888888888888, 'support': 13}, 'class_4': {'precision': 1.0, 'recall': 1.0, 'f1': 1.0, 'support': 8}}, 'eval_runtime': 1.4639, 'eval_samples_per_second': 34.155, 'eval_steps_per_second': 4.782, 'epoch': 8.0}\n",
            "\n",
            "======= Overall Metrics =======\n",
            "Accuracy: 0.940\n",
            "Macro Precision: 0.953\n",
            "Macro Recall: 0.941\n",
            "Macro F1-score: 0.946\n",
            "\n",
            "======= Metrics for each Class =======\n",
            "         precision  recall    f1  support\n",
            "class_0      1.000   0.875 0.933        8\n",
            "class_1      1.000   1.000 1.000       10\n",
            "class_2      0.909   0.909 0.909       11\n",
            "class_3      0.857   0.923 0.889       13\n",
            "class_4      1.000   1.000 1.000        8\n",
            "Training roberta-base...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-18-17b176282799>:86: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='152' max='152' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [152/152 05:21, Epoch 8/8]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>Macro Precision</th>\n",
              "      <th>Macro Recall</th>\n",
              "      <th>Macro F1</th>\n",
              "      <th>Per Class Metrics</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>No log</td>\n",
              "      <td>1.432471</td>\n",
              "      <td>0.360000</td>\n",
              "      <td>0.197089</td>\n",
              "      <td>0.400000</td>\n",
              "      <td>0.245024</td>\n",
              "      <td>{'class_0': {'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'support': 8}, 'class_1': {'precision': 0.7692307692307693, 'recall': 1.0, 'f1': 0.8695652173913043, 'support': 10}, 'class_2': {'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'support': 11}, 'class_3': {'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'support': 13}, 'class_4': {'precision': 0.21621621621621623, 'recall': 1.0, 'f1': 0.35555555555555557, 'support': 8}}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.950661</td>\n",
              "      <td>0.620000</td>\n",
              "      <td>0.597647</td>\n",
              "      <td>0.627972</td>\n",
              "      <td>0.532143</td>\n",
              "      <td>{'class_0': {'precision': 0.4, 'recall': 1.0, 'f1': 0.5714285714285714, 'support': 8}, 'class_1': {'precision': 1.0, 'recall': 1.0, 'f1': 1.0, 'support': 10}, 'class_2': {'precision': 0.5882352941176471, 'recall': 0.9090909090909091, 'f1': 0.7142857142857143, 'support': 11}, 'class_3': {'precision': 1.0, 'recall': 0.23076923076923078, 'f1': 0.375, 'support': 13}, 'class_4': {'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'support': 8}}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.608403</td>\n",
              "      <td>0.820000</td>\n",
              "      <td>0.898810</td>\n",
              "      <td>0.802273</td>\n",
              "      <td>0.816830</td>\n",
              "      <td>{'class_0': {'precision': 0.875, 'recall': 0.875, 'f1': 0.875, 'support': 8}, 'class_1': {'precision': 1.0, 'recall': 1.0, 'f1': 1.0, 'support': 10}, 'class_2': {'precision': 1.0, 'recall': 0.6363636363636364, 'f1': 0.7777777777777778, 'support': 11}, 'class_3': {'precision': 0.6190476190476191, 'recall': 1.0, 'f1': 0.7647058823529411, 'support': 13}, 'class_4': {'precision': 1.0, 'recall': 0.5, 'f1': 0.6666666666666666, 'support': 8}}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.400563</td>\n",
              "      <td>0.920000</td>\n",
              "      <td>0.925714</td>\n",
              "      <td>0.906818</td>\n",
              "      <td>0.910053</td>\n",
              "      <td>{'class_0': {'precision': 0.7, 'recall': 0.875, 'f1': 0.7777777777777778, 'support': 8}, 'class_1': {'precision': 1.0, 'recall': 1.0, 'f1': 1.0, 'support': 10}, 'class_2': {'precision': 1.0, 'recall': 0.9090909090909091, 'f1': 0.9523809523809523, 'support': 11}, 'class_3': {'precision': 0.9285714285714286, 'recall': 1.0, 'f1': 0.9629629629629629, 'support': 13}, 'class_4': {'precision': 1.0, 'recall': 0.75, 'f1': 0.8571428571428571, 'support': 8}}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.502409</td>\n",
              "      <td>0.840000</td>\n",
              "      <td>0.901429</td>\n",
              "      <td>0.806818</td>\n",
              "      <td>0.817143</td>\n",
              "      <td>{'class_0': {'precision': 0.8571428571428571, 'recall': 0.75, 'f1': 0.8, 'support': 8}, 'class_1': {'precision': 1.0, 'recall': 1.0, 'f1': 1.0, 'support': 10}, 'class_2': {'precision': 1.0, 'recall': 0.9090909090909091, 'f1': 0.9523809523809523, 'support': 11}, 'class_3': {'precision': 0.65, 'recall': 1.0, 'f1': 0.7878787878787878, 'support': 13}, 'class_4': {'precision': 1.0, 'recall': 0.375, 'f1': 0.5454545454545454, 'support': 8}}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.591274</td>\n",
              "      <td>0.840000</td>\n",
              "      <td>0.886842</td>\n",
              "      <td>0.806818</td>\n",
              "      <td>0.812067</td>\n",
              "      <td>{'class_0': {'precision': 0.75, 'recall': 0.75, 'f1': 0.75, 'support': 8}, 'class_1': {'precision': 1.0, 'recall': 1.0, 'f1': 1.0, 'support': 10}, 'class_2': {'precision': 1.0, 'recall': 0.9090909090909091, 'f1': 0.9523809523809523, 'support': 11}, 'class_3': {'precision': 0.6842105263157895, 'recall': 1.0, 'f1': 0.8125, 'support': 13}, 'class_4': {'precision': 1.0, 'recall': 0.375, 'f1': 0.5454545454545454, 'support': 8}}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.506715</td>\n",
              "      <td>0.880000</td>\n",
              "      <td>0.891818</td>\n",
              "      <td>0.866434</td>\n",
              "      <td>0.874675</td>\n",
              "      <td>{'class_0': {'precision': 0.75, 'recall': 0.75, 'f1': 0.75, 'support': 8}, 'class_1': {'precision': 1.0, 'recall': 1.0, 'f1': 1.0, 'support': 10}, 'class_2': {'precision': 0.9090909090909091, 'recall': 0.9090909090909091, 'f1': 0.9090909090909091, 'support': 11}, 'class_3': {'precision': 0.8, 'recall': 0.9230769230769231, 'f1': 0.8571428571428571, 'support': 13}, 'class_4': {'precision': 1.0, 'recall': 0.75, 'f1': 0.8571428571428571, 'support': 8}}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.509621</td>\n",
              "      <td>0.900000</td>\n",
              "      <td>0.908802</td>\n",
              "      <td>0.891434</td>\n",
              "      <td>0.895730</td>\n",
              "      <td>{'class_0': {'precision': 0.7777777777777778, 'recall': 0.875, 'f1': 0.8235294117647058, 'support': 8}, 'class_1': {'precision': 1.0, 'recall': 1.0, 'f1': 1.0, 'support': 10}, 'class_2': {'precision': 0.9090909090909091, 'recall': 0.9090909090909091, 'f1': 0.9090909090909091, 'support': 11}, 'class_3': {'precision': 0.8571428571428571, 'recall': 0.9230769230769231, 'f1': 0.8888888888888888, 'support': 13}, 'class_4': {'precision': 1.0, 'recall': 0.75, 'f1': 0.8571428571428571, 'support': 8}}</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='7' max='7' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [7/7 00:01]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluation results for roberta-base:\n",
            "{'eval_loss': 0.4005626440048218, 'eval_accuracy': 0.92, 'eval_macro_precision': 0.9257142857142858, 'eval_macro_recall': 0.9068181818181819, 'eval_macro_f1': 0.91005291005291, 'eval_per_class_metrics': {'class_0': {'precision': 0.7, 'recall': 0.875, 'f1': 0.7777777777777778, 'support': 8}, 'class_1': {'precision': 1.0, 'recall': 1.0, 'f1': 1.0, 'support': 10}, 'class_2': {'precision': 1.0, 'recall': 0.9090909090909091, 'f1': 0.9523809523809523, 'support': 11}, 'class_3': {'precision': 0.9285714285714286, 'recall': 1.0, 'f1': 0.9629629629629629, 'support': 13}, 'class_4': {'precision': 1.0, 'recall': 0.75, 'f1': 0.8571428571428571, 'support': 8}}, 'eval_runtime': 1.3706, 'eval_samples_per_second': 36.481, 'eval_steps_per_second': 5.107, 'epoch': 8.0}\n",
            "\n",
            "======= Overall Metrics =======\n",
            "Accuracy: 0.920\n",
            "Macro Precision: 0.926\n",
            "Macro Recall: 0.907\n",
            "Macro F1-score: 0.910\n",
            "\n",
            "======= Metrics for each Class =======\n",
            "         precision  recall    f1  support\n",
            "class_0      0.700   0.875 0.778        8\n",
            "class_1      1.000   1.000 1.000       10\n",
            "class_2      1.000   0.909 0.952       11\n",
            "class_3      0.929   1.000 0.963       13\n",
            "class_4      1.000   0.750 0.857        8\n",
            "Training distilbert-base-uncased...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-18-17b176282799>:86: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='152' max='152' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [152/152 01:58, Epoch 8/8]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>Macro Precision</th>\n",
              "      <th>Macro Recall</th>\n",
              "      <th>Macro F1</th>\n",
              "      <th>Per Class Metrics</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>No log</td>\n",
              "      <td>1.224218</td>\n",
              "      <td>0.620000</td>\n",
              "      <td>0.767018</td>\n",
              "      <td>0.620804</td>\n",
              "      <td>0.589501</td>\n",
              "      <td>{'class_0': {'precision': 0.3684210526315789, 'recall': 0.875, 'f1': 0.5185185185185185, 'support': 8}, 'class_1': {'precision': 0.6666666666666666, 'recall': 1.0, 'f1': 0.8, 'support': 10}, 'class_2': {'precision': 1.0, 'recall': 0.36363636363636365, 'f1': 0.5333333333333333, 'support': 11}, 'class_3': {'precision': 0.8, 'recall': 0.6153846153846154, 'f1': 0.6956521739130435, 'support': 13}, 'class_4': {'precision': 1.0, 'recall': 0.25, 'f1': 0.4, 'support': 8}}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.757120</td>\n",
              "      <td>0.800000</td>\n",
              "      <td>0.829524</td>\n",
              "      <td>0.785664</td>\n",
              "      <td>0.788728</td>\n",
              "      <td>{'class_0': {'precision': 0.6, 'recall': 0.75, 'f1': 0.6666666666666666, 'support': 8}, 'class_1': {'precision': 1.0, 'recall': 1.0, 'f1': 1.0, 'support': 10}, 'class_2': {'precision': 0.8333333333333334, 'recall': 0.9090909090909091, 'f1': 0.8695652173913043, 'support': 11}, 'class_3': {'precision': 0.7142857142857143, 'recall': 0.7692307692307693, 'f1': 0.7407407407407407, 'support': 13}, 'class_4': {'precision': 1.0, 'recall': 0.5, 'f1': 0.6666666666666666, 'support': 8}}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.429635</td>\n",
              "      <td>0.920000</td>\n",
              "      <td>0.938462</td>\n",
              "      <td>0.919231</td>\n",
              "      <td>0.923993</td>\n",
              "      <td>{'class_0': {'precision': 1.0, 'recall': 0.75, 'f1': 0.8571428571428571, 'support': 8}, 'class_1': {'precision': 1.0, 'recall': 1.0, 'f1': 1.0, 'support': 10}, 'class_2': {'precision': 0.8461538461538461, 'recall': 1.0, 'f1': 0.9166666666666666, 'support': 11}, 'class_3': {'precision': 0.8461538461538461, 'recall': 0.8461538461538461, 'f1': 0.8461538461538461, 'support': 13}, 'class_4': {'precision': 1.0, 'recall': 1.0, 'f1': 1.0, 'support': 8}}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.328490</td>\n",
              "      <td>0.920000</td>\n",
              "      <td>0.943333</td>\n",
              "      <td>0.909615</td>\n",
              "      <td>0.920828</td>\n",
              "      <td>{'class_0': {'precision': 1.0, 'recall': 0.75, 'f1': 0.8571428571428571, 'support': 8}, 'class_1': {'precision': 1.0, 'recall': 1.0, 'f1': 1.0, 'support': 10}, 'class_2': {'precision': 0.9166666666666666, 'recall': 1.0, 'f1': 0.9565217391304348, 'support': 11}, 'class_3': {'precision': 0.8, 'recall': 0.9230769230769231, 'f1': 0.8571428571428571, 'support': 13}, 'class_4': {'precision': 1.0, 'recall': 0.875, 'f1': 0.9333333333333333, 'support': 8}}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.256661</td>\n",
              "      <td>0.940000</td>\n",
              "      <td>0.942949</td>\n",
              "      <td>0.934615</td>\n",
              "      <td>0.937586</td>\n",
              "      <td>{'class_0': {'precision': 0.875, 'recall': 0.875, 'f1': 0.875, 'support': 8}, 'class_1': {'precision': 1.0, 'recall': 1.0, 'f1': 1.0, 'support': 10}, 'class_2': {'precision': 0.9166666666666666, 'recall': 1.0, 'f1': 0.9565217391304348, 'support': 11}, 'class_3': {'precision': 0.9230769230769231, 'recall': 0.9230769230769231, 'f1': 0.9230769230769231, 'support': 13}, 'class_4': {'precision': 1.0, 'recall': 0.875, 'f1': 0.9333333333333333, 'support': 8}}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.227749</td>\n",
              "      <td>0.940000</td>\n",
              "      <td>0.952564</td>\n",
              "      <td>0.944231</td>\n",
              "      <td>0.946000</td>\n",
              "      <td>{'class_0': {'precision': 1.0, 'recall': 0.875, 'f1': 0.9333333333333333, 'support': 8}, 'class_1': {'precision': 1.0, 'recall': 1.0, 'f1': 1.0, 'support': 10}, 'class_2': {'precision': 0.8461538461538461, 'recall': 1.0, 'f1': 0.9166666666666666, 'support': 11}, 'class_3': {'precision': 0.9166666666666666, 'recall': 0.8461538461538461, 'f1': 0.88, 'support': 13}, 'class_4': {'precision': 1.0, 'recall': 1.0, 'f1': 1.0, 'support': 8}}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.222437</td>\n",
              "      <td>0.960000</td>\n",
              "      <td>0.967949</td>\n",
              "      <td>0.959615</td>\n",
              "      <td>0.962586</td>\n",
              "      <td>{'class_0': {'precision': 1.0, 'recall': 0.875, 'f1': 0.9333333333333333, 'support': 8}, 'class_1': {'precision': 1.0, 'recall': 1.0, 'f1': 1.0, 'support': 10}, 'class_2': {'precision': 0.9166666666666666, 'recall': 1.0, 'f1': 0.9565217391304348, 'support': 11}, 'class_3': {'precision': 0.9230769230769231, 'recall': 0.9230769230769231, 'f1': 0.9230769230769231, 'support': 13}, 'class_4': {'precision': 1.0, 'recall': 1.0, 'f1': 1.0, 'support': 8}}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.220338</td>\n",
              "      <td>0.960000</td>\n",
              "      <td>0.967949</td>\n",
              "      <td>0.959615</td>\n",
              "      <td>0.962586</td>\n",
              "      <td>{'class_0': {'precision': 1.0, 'recall': 0.875, 'f1': 0.9333333333333333, 'support': 8}, 'class_1': {'precision': 1.0, 'recall': 1.0, 'f1': 1.0, 'support': 10}, 'class_2': {'precision': 0.9166666666666666, 'recall': 1.0, 'f1': 0.9565217391304348, 'support': 11}, 'class_3': {'precision': 0.9230769230769231, 'recall': 0.9230769230769231, 'f1': 0.9230769230769231, 'support': 13}, 'class_4': {'precision': 1.0, 'recall': 1.0, 'f1': 1.0, 'support': 8}}</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='7' max='7' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [7/7 00:00]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluation results for distilbert-base-uncased:\n",
            "{'eval_loss': 0.22243747115135193, 'eval_accuracy': 0.96, 'eval_macro_precision': 0.9679487179487178, 'eval_macro_recall': 0.9596153846153846, 'eval_macro_f1': 0.9625863991081383, 'eval_per_class_metrics': {'class_0': {'precision': 1.0, 'recall': 0.875, 'f1': 0.9333333333333333, 'support': 8}, 'class_1': {'precision': 1.0, 'recall': 1.0, 'f1': 1.0, 'support': 10}, 'class_2': {'precision': 0.9166666666666666, 'recall': 1.0, 'f1': 0.9565217391304348, 'support': 11}, 'class_3': {'precision': 0.9230769230769231, 'recall': 0.9230769230769231, 'f1': 0.9230769230769231, 'support': 13}, 'class_4': {'precision': 1.0, 'recall': 1.0, 'f1': 1.0, 'support': 8}}, 'eval_runtime': 0.7766, 'eval_samples_per_second': 64.387, 'eval_steps_per_second': 9.014, 'epoch': 8.0}\n",
            "\n",
            "======= Overall Metrics =======\n",
            "Accuracy: 0.960\n",
            "Macro Precision: 0.968\n",
            "Macro Recall: 0.960\n",
            "Macro F1-score: 0.963\n",
            "\n",
            "======= Metrics for each Class =======\n",
            "         precision  recall    f1  support\n",
            "class_0      1.000   0.875 0.933        8\n",
            "class_1      1.000   1.000 1.000       10\n",
            "class_2      0.917   1.000 0.957       11\n",
            "class_3      0.923   0.923 0.923       13\n",
            "class_4      1.000   1.000 1.000        8\n",
            "Training microsoft/BiomedNLP-BiomedBERT-base-uncased-abstract...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at microsoft/BiomedNLP-BiomedBERT-base-uncased-abstract and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-18-17b176282799>:86: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='152' max='152' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [152/152 05:45, Epoch 8/8]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>Macro Precision</th>\n",
              "      <th>Macro Recall</th>\n",
              "      <th>Macro F1</th>\n",
              "      <th>Per Class Metrics</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>No log</td>\n",
              "      <td>1.290194</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.478022</td>\n",
              "      <td>0.546154</td>\n",
              "      <td>0.431710</td>\n",
              "      <td>{'class_0': {'precision': 0.46153846153846156, 'recall': 0.75, 'f1': 0.5714285714285714, 'support': 8}, 'class_1': {'precision': 0.5, 'recall': 1.0, 'f1': 0.6666666666666666, 'support': 10}, 'class_2': {'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'support': 11}, 'class_3': {'precision': 1.0, 'recall': 0.23076923076923078, 'f1': 0.375, 'support': 13}, 'class_4': {'precision': 0.42857142857142855, 'recall': 0.75, 'f1': 0.5454545454545454, 'support': 8}}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.758283</td>\n",
              "      <td>0.780000</td>\n",
              "      <td>0.792757</td>\n",
              "      <td>0.787937</td>\n",
              "      <td>0.776792</td>\n",
              "      <td>{'class_0': {'precision': 0.8571428571428571, 'recall': 0.75, 'f1': 0.8, 'support': 8}, 'class_1': {'precision': 0.9090909090909091, 'recall': 1.0, 'f1': 0.9523809523809523, 'support': 10}, 'class_2': {'precision': 0.75, 'recall': 0.5454545454545454, 'f1': 0.631578947368421, 'support': 11}, 'class_3': {'precision': 0.9090909090909091, 'recall': 0.7692307692307693, 'f1': 0.8333333333333334, 'support': 13}, 'class_4': {'precision': 0.5384615384615384, 'recall': 0.875, 'f1': 0.6666666666666666, 'support': 8}}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.579439</td>\n",
              "      <td>0.820000</td>\n",
              "      <td>0.811328</td>\n",
              "      <td>0.807867</td>\n",
              "      <td>0.807760</td>\n",
              "      <td>{'class_0': {'precision': 0.8571428571428571, 'recall': 0.75, 'f1': 0.8, 'support': 8}, 'class_1': {'precision': 0.9090909090909091, 'recall': 1.0, 'f1': 0.9523809523809523, 'support': 10}, 'class_2': {'precision': 0.8181818181818182, 'recall': 0.8181818181818182, 'f1': 0.8181818181818182, 'support': 11}, 'class_3': {'precision': 0.9166666666666666, 'recall': 0.8461538461538461, 'f1': 0.88, 'support': 13}, 'class_4': {'precision': 0.5555555555555556, 'recall': 0.625, 'f1': 0.5882352941176471, 'support': 8}}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.623060</td>\n",
              "      <td>0.780000</td>\n",
              "      <td>0.753247</td>\n",
              "      <td>0.748252</td>\n",
              "      <td>0.739182</td>\n",
              "      <td>{'class_0': {'precision': 0.8571428571428571, 'recall': 0.75, 'f1': 0.8, 'support': 8}, 'class_1': {'precision': 0.9090909090909091, 'recall': 1.0, 'f1': 0.9523809523809523, 'support': 10}, 'class_2': {'precision': 0.75, 'recall': 0.8181818181818182, 'f1': 0.782608695652174, 'support': 11}, 'class_3': {'precision': 0.75, 'recall': 0.9230769230769231, 'f1': 0.8275862068965517, 'support': 13}, 'class_4': {'precision': 0.5, 'recall': 0.25, 'f1': 0.3333333333333333, 'support': 8}}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.678452</td>\n",
              "      <td>0.840000</td>\n",
              "      <td>0.830037</td>\n",
              "      <td>0.816434</td>\n",
              "      <td>0.818730</td>\n",
              "      <td>{'class_0': {'precision': 0.8571428571428571, 'recall': 0.75, 'f1': 0.8, 'support': 8}, 'class_1': {'precision': 1.0, 'recall': 1.0, 'f1': 1.0, 'support': 10}, 'class_2': {'precision': 0.7692307692307693, 'recall': 0.9090909090909091, 'f1': 0.8333333333333334, 'support': 11}, 'class_3': {'precision': 0.8571428571428571, 'recall': 0.9230769230769231, 'f1': 0.8888888888888888, 'support': 13}, 'class_4': {'precision': 0.6666666666666666, 'recall': 0.5, 'f1': 0.5714285714285714, 'support': 8}}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.675570</td>\n",
              "      <td>0.860000</td>\n",
              "      <td>0.860275</td>\n",
              "      <td>0.841434</td>\n",
              "      <td>0.842521</td>\n",
              "      <td>{'class_0': {'precision': 0.875, 'recall': 0.875, 'f1': 0.875, 'support': 8}, 'class_1': {'precision': 1.0, 'recall': 1.0, 'f1': 1.0, 'support': 10}, 'class_2': {'precision': 0.7692307692307693, 'recall': 0.9090909090909091, 'f1': 0.8333333333333334, 'support': 11}, 'class_3': {'precision': 0.8571428571428571, 'recall': 0.9230769230769231, 'f1': 0.8888888888888888, 'support': 13}, 'class_4': {'precision': 0.8, 'recall': 0.5, 'f1': 0.6153846153846154, 'support': 8}}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.712104</td>\n",
              "      <td>0.860000</td>\n",
              "      <td>0.854017</td>\n",
              "      <td>0.841434</td>\n",
              "      <td>0.839065</td>\n",
              "      <td>{'class_0': {'precision': 0.7777777777777778, 'recall': 0.875, 'f1': 0.8235294117647058, 'support': 8}, 'class_1': {'precision': 1.0, 'recall': 1.0, 'f1': 1.0, 'support': 10}, 'class_2': {'precision': 0.7692307692307693, 'recall': 0.9090909090909091, 'f1': 0.8333333333333334, 'support': 11}, 'class_3': {'precision': 0.9230769230769231, 'recall': 0.9230769230769231, 'f1': 0.9230769230769231, 'support': 13}, 'class_4': {'precision': 0.8, 'recall': 0.5, 'f1': 0.6153846153846154, 'support': 8}}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.716519</td>\n",
              "      <td>0.840000</td>\n",
              "      <td>0.821795</td>\n",
              "      <td>0.816434</td>\n",
              "      <td>0.815568</td>\n",
              "      <td>{'class_0': {'precision': 0.75, 'recall': 0.75, 'f1': 0.75, 'support': 8}, 'class_1': {'precision': 1.0, 'recall': 1.0, 'f1': 1.0, 'support': 10}, 'class_2': {'precision': 0.7692307692307693, 'recall': 0.9090909090909091, 'f1': 0.8333333333333334, 'support': 11}, 'class_3': {'precision': 0.9230769230769231, 'recall': 0.9230769230769231, 'f1': 0.9230769230769231, 'support': 13}, 'class_4': {'precision': 0.6666666666666666, 'recall': 0.5, 'f1': 0.5714285714285714, 'support': 8}}</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='7' max='7' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [7/7 00:01]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluation results for microsoft/BiomedNLP-BiomedBERT-base-uncased-abstract:\n",
            "{'eval_loss': 0.6755703091621399, 'eval_accuracy': 0.86, 'eval_macro_precision': 0.8602747252747254, 'eval_macro_recall': 0.8414335664335665, 'eval_macro_f1': 0.8425213675213676, 'eval_per_class_metrics': {'class_0': {'precision': 0.875, 'recall': 0.875, 'f1': 0.875, 'support': 8}, 'class_1': {'precision': 1.0, 'recall': 1.0, 'f1': 1.0, 'support': 10}, 'class_2': {'precision': 0.7692307692307693, 'recall': 0.9090909090909091, 'f1': 0.8333333333333334, 'support': 11}, 'class_3': {'precision': 0.8571428571428571, 'recall': 0.9230769230769231, 'f1': 0.8888888888888888, 'support': 13}, 'class_4': {'precision': 0.8, 'recall': 0.5, 'f1': 0.6153846153846154, 'support': 8}}, 'eval_runtime': 1.5254, 'eval_samples_per_second': 32.779, 'eval_steps_per_second': 4.589, 'epoch': 8.0}\n",
            "\n",
            "======= Overall Metrics =======\n",
            "Accuracy: 0.860\n",
            "Macro Precision: 0.860\n",
            "Macro Recall: 0.841\n",
            "Macro F1-score: 0.843\n",
            "\n",
            "======= Metrics for each Class =======\n",
            "         precision  recall    f1  support\n",
            "class_0      0.875   0.875 0.875        8\n",
            "class_1      1.000   1.000 1.000       10\n",
            "class_2      0.769   0.909 0.833       11\n",
            "class_3      0.857   0.923 0.889       13\n",
            "class_4      0.800   0.500 0.615        8\n",
            "\n",
            "====== Best Models Summary ======\n",
            "Model: bert-base-uncased | Best Path: model_bert-base-uncased/best_model | Best Macro-F1: 0.946\n",
            "Model: roberta-base | Best Path: model_roberta-base/best_model | Best Macro-F1: 0.910\n",
            "Model: distilbert-base-uncased | Best Path: model_distilbert-base-uncased/best_model | Best Macro-F1: 0.963\n",
            "Model: microsoft/BiomedNLP-BiomedBERT-base-uncased-abstract | Best Path: model_microsoft/BiomedNLP-BiomedBERT-base-uncased-abstract/best_model | Best Macro-F1: 0.843\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import torch\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForSequenceClassification,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        "    DataCollatorWithPadding\n",
        ")\n",
        "from datasets import Dataset\n",
        "import pandas as pd\n",
        "\n",
        "SEED = 111\n",
        "\n",
        "# Fix Python, NumPy, PyTorch, and CUDA randomness\n",
        "os.environ[\"PYTHONHASHSEED\"] = str(SEED)\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed(SEED)\n",
        "torch.cuda.manual_seed_all(SEED)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "\n",
        "base_models = [\n",
        "    'bert-base-uncased',\n",
        "    'roberta-base',\n",
        "    'distilbert-base-uncased',\n",
        "    'microsoft/BiomedNLP-BiomedBERT-base-uncased-abstract'\n",
        "]\n",
        "\n",
        "# Corrected label mapping\n",
        "label_mapping = {0: 4, 1: 3, 2: 2, 3: 1, 4: 0}\n",
        "val_labels_fixed = [label_mapping[label] for label in val_labels]\n",
        "\n",
        "# Re-create the original datasets with the 'text' column\n",
        "train_texts = [record[\"content\"] for record in museums[\"train\"]]\n",
        "train_labels = [record[\"label\"] for record in museums[\"train\"]]\n",
        "val_texts = [record[\"content\"] for record in museums[\"val\"]]\n",
        "\n",
        "train_dataset_original = Dataset.from_dict({\"text\": train_texts, \"label\": train_labels})\n",
        "val_dataset_original = Dataset.from_dict({\"text\": val_texts, \"label\": val_labels_fixed})\n",
        "\n",
        "# Dictionaries to store best models and scores\n",
        "best_model_paths = {}  # To store the best checkpoint directory per model\n",
        "best_model_scores = {}  # To store the best macro-F1 score per model\n",
        "\n",
        "for model_name in base_models:\n",
        "    print(f\"Training {model_name}...\")\n",
        "\n",
        "    # Load tokenizer and model\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=num_labels)\n",
        "\n",
        "    # Tokenize datasets - Using the original datasets with the 'text' column\n",
        "    train_dataset = train_dataset_original.map(\n",
        "        lambda examples: tokenizer(examples['text'], padding=\"max_length\", truncation=True, max_length=512),\n",
        "        batched=True\n",
        "    )\n",
        "    val_dataset = val_dataset_original.map(\n",
        "        lambda examples: tokenizer(examples['text'], padding=\"max_length\", truncation=True, max_length=512),\n",
        "        batched=True\n",
        "    )\n",
        "\n",
        "    # Define output directory\n",
        "    output_dir = f\"model_{model_name}\"\n",
        "\n",
        "    # Define training arguments with load_best_model_at_end\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=output_dir,\n",
        "        # Corrected parameter name from evaluation_strategy to eval_strategy\n",
        "        eval_strategy=\"epoch\",\n",
        "        save_strategy=\"epoch\",\n",
        "        save_total_limit=1,  # Keep only the best checkpoint\n",
        "        load_best_model_at_end=True,  # Load best model\n",
        "        metric_for_best_model=\"eval_macro_f1\",  # Evaluate based on macro F1\n",
        "        greater_is_better=True,  # Higher F1 is better\n",
        "        per_device_train_batch_size=8,\n",
        "        per_device_eval_batch_size=8,\n",
        "        num_train_epochs=8,\n",
        "        learning_rate=5e-5,\n",
        "        report_to=\"none\"\n",
        "    )\n",
        "\n",
        "    # Create Trainer\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=train_dataset,\n",
        "        eval_dataset=val_dataset,\n",
        "        data_collator=DataCollatorWithPadding(tokenizer=tokenizer),\n",
        "        tokenizer=tokenizer,\n",
        "        compute_metrics=compute_metrics\n",
        "    )\n",
        "\n",
        "    # Train the model\n",
        "    trainer.train()\n",
        "\n",
        "    # Save the best model (ensured by load_best_model_at_end)\n",
        "    best_model_path = os.path.join(output_dir, \"best_model\")\n",
        "    trainer.save_model(best_model_path)\n",
        "\n",
        "    # Evaluate the best model\n",
        "    eval_results = trainer.evaluate()\n",
        "    print(f\"Evaluation results for {model_name}:\")\n",
        "    print(eval_results)\n",
        "\n",
        "    # Store the best model path and score in variables (not CSV)\n",
        "    best_model_paths[model_name] = best_model_path\n",
        "    best_model_scores[model_name] = eval_results[\"eval_macro_f1\"]\n",
        "\n",
        "    # Print overall and per-class metrics\n",
        "    overall_metrics = {\n",
        "        \"Accuracy\": eval_results[\"eval_accuracy\"],\n",
        "        \"Macro Precision\": eval_results[\"eval_macro_precision\"],\n",
        "        \"Macro Recall\": eval_results[\"eval_macro_recall\"],\n",
        "        \"Macro F1-score\": eval_results[\"eval_macro_f1\"]\n",
        "    }\n",
        "\n",
        "    # Note: The compute_metrics function returns 'per_class_metrics' under the key 'per_class_metrics'.\n",
        "    # The trainer adds 'eval_' prefix. So, access it as 'eval_per_class_metrics'.\n",
        "    # Assuming compute_metrics returns a dict with the key 'per_class_metrics'.\n",
        "    # We need to check the exact structure returned by trainer.evaluate()\n",
        "    # Based on the previous cell, it seems the keys are directly prefixed with 'eval_'.\n",
        "    # Let's refine how to extract the per-class metrics.\n",
        "    # The previous cell correctly extracted them by filtering keys starting with 'eval_per_class_metrics_'.\n",
        "    # Let's revert to that approach if needed, but the simplest way is to check the structure of eval_results\n",
        "    # and access the key that holds the per_class_metrics dictionary.\n",
        "    # Let's assume it's directly eval_results[\"eval_per_class_metrics\"] as the compute_metrics returns 'per_class_metrics'\n",
        "    # and trainer prefixes 'eval_'.\n",
        "\n",
        "    # Access the per_class_metrics dictionary directly from eval_results\n",
        "    per_class_data = eval_results.get(\"eval_per_class_metrics\", {})\n",
        "\n",
        "    # Convert per-class metrics to DataFrame\n",
        "    if per_class_data:\n",
        "        per_class_metrics_df = pd.DataFrame.from_dict(per_class_data, orient=\"index\")\n",
        "    else:\n",
        "        per_class_metrics_df = pd.DataFrame() # Create empty DataFrame if no metrics are found\n",
        "\n",
        "    # Print Overall Metrics\n",
        "    print(\"\\n======= Overall Metrics =======\")\n",
        "    for metric, value in overall_metrics.items():\n",
        "        print(f\"{metric}: {value:.3f}\")\n",
        "\n",
        "    # Print Per-Class Metrics in Table Format\n",
        "    print(\"\\n======= Metrics for each Class =======\")\n",
        "    if not per_class_metrics_df.empty:\n",
        "        print(per_class_metrics_df.to_string(float_format=\"%.3f\"))\n",
        "    else:\n",
        "        print(\"No per-class metrics available.\")\n",
        "\n",
        "# Print Summary of Best Models (No CSV, just stored in variables)\n",
        "print(\"\\n====== Best Models Summary ======\")\n",
        "for model, path in best_model_paths.items():\n",
        "    print(f\"Model: {model} | Best Path: {path} | Best Macro-F1: {best_model_scores[model]:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r1bbgLU5_ETf"
      },
      "source": [
        "##Final evaluation and deployment\n",
        "Load the best model (based on macro-F1 on the validation set) that was saved in the previous question using a\n",
        "\u2018text-classification\u2019 pipeline. Evaluate the best of the four fine-tuned models on the testing set.\n",
        "State which model you used and report the per-class precision, recall and F1 score as well as the accuracy, macro\n",
        "precision, macro recall and macro F1 score. Comment on the performance and discuss whether the quality is high\n",
        "enough to be deployed for the client."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9TOp4WQY_KDd",
        "outputId": "be65d444-82c3-42b4-edaf-834db735470c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading best model: distilbert-base-uncased from model_distilbert-base-uncased/best_model\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final Model Evaluation on Test Set:\n",
            "Best Model: distilbert-base-uncased\n",
            "\n",
            "                                       precision  recall  f1-score\n",
            "National Maritime Museum                  1.0000  0.8462    0.9167\n",
            "National Railway Museum                   0.8750  1.0000    0.9333\n",
            "Royal Botanic Gardens, Kew                0.8462  1.0000    0.9167\n",
            "Royal College of Physicians of London     0.8571  1.0000    0.9231\n",
            "Shakespeare Birthplace Trust              0.9091  0.7692    0.8333\n",
            "accuracy                                  0.9000  0.9000    0.9000\n",
            "macro avg                                 0.8975  0.9231    0.9046\n",
            "weighted avg                              0.9079  0.9000    0.8981\n",
            "\n",
            "====== Overall Metrics ======\n",
            "Accuracy: 0.9000\n",
            "Macro Precision: 0.8975\n",
            "Macro Recall: 0.9231\n",
            "Macro F1-score: 0.9046\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline\n",
        "from datasets import Dataset\n",
        "from sklearn.metrics import classification_report, precision_score, recall_score, f1_score\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Load the best model based on macro-F1 on the validation set\n",
        "best_model_name = max(best_model_scores, key=best_model_scores.get)\n",
        "best_model_path = best_model_paths[best_model_name]\n",
        "\n",
        "print(f\"Loading best model: {best_model_name} from {best_model_path}\\n\")\n",
        "\n",
        "# Load tokenizer and model\n",
        "classifier = pipeline(\"text-classification\", model=best_model_path, tokenizer=best_model_path)\n",
        "\n",
        "# Load the test dataset\n",
        "test_texts = [record[\"content\"] for record in museums[\"test\"]]\n",
        "test_labels = [record[\"label\"] for record in museums[\"test\"]]\n",
        "\n",
        "# Make predictions\n",
        "predictions = classifier(test_texts, truncation=True)\n",
        "predicted_labels = [int(pred[\"label\"].split(\"_\")[-1]) for pred in predictions]\n",
        "\n",
        "# Generate classification report (excluding 'support')\n",
        "report_dict = classification_report(test_labels, predicted_labels, target_names=[\n",
        "    \"National Maritime Museum\", \"National Railway Museum\", \"Royal Botanic Gardens, Kew\",\n",
        "    \"Royal College of Physicians of London\", \"Shakespeare Birthplace Trust\"\n",
        "], digits=4, output_dict=True, zero_division=0)  # zero_division handles potential errors\n",
        "\n",
        "# Remove 'support' from the report dictionary\n",
        "for key in report_dict:\n",
        "    if isinstance(report_dict[key], dict) and 'support' in report_dict[key]:\n",
        "        del report_dict[key]['support']\n",
        "\n",
        "# Extracting values for macro precision, recall, and F1-score\n",
        "macro_precision = report_dict[\"macro avg\"][\"precision\"]\n",
        "macro_recall = report_dict[\"macro avg\"][\"recall\"]\n",
        "macro_f1 = report_dict[\"macro avg\"][\"f1-score\"]\n",
        "\n",
        "# Extracting accuracy\n",
        "accuracy = report_dict[\"accuracy\"]\n",
        "\n",
        "# Extracting per-class metrics (excluding 'support')\n",
        "per_class_metrics = pd.DataFrame(report_dict).transpose().drop('support', axis=1, errors='ignore')\n",
        "\n",
        "# Print results\n",
        "print(\"Final Model Evaluation on Test Set:\")\n",
        "print(f\"Best Model: {best_model_name}\\n\")  # State the model used\n",
        "print(per_class_metrics.to_string(float_format=\"%.4f\"))\n",
        "\n",
        "print(\"\\n====== Overall Metrics ======\")\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "print(f\"Macro Precision: {macro_precision:.4f}\")\n",
        "print(f\"Macro Recall: {macro_recall:.4f}\")\n",
        "print(f\"Macro F1-score: {macro_f1:.4f}\\n\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}